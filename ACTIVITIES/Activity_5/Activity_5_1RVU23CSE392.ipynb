{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "971ee1f1-9699-44c4-849e-15ede3684f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter \n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78bb12f4-24f1-4406-bfeb-b9aecf5ecfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=open(\"C:/Users/Lenovo/Downloads/GenAl/Activity_5/corpus.txt\").read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef0ebc3-7437-44cb-aeb8-55c571b41cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1deee9bc-a459-49c2-aa6d-c490728866a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc327d17-5699-4cbc-afe5-31df9f558764",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_counts = Counter()\n",
    "word_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02ae8c2a-2d78-4e68-98ba-ea0406b9a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigrams(words):\n",
    "    return list(zip(words, islice(words, 1, None), islice(words, 2, None)))\n",
    "\n",
    "processed_sentences = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    words = word_tokenize(sentence)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in filtered_words]\n",
    "    \n",
    "    word_counts.update(lemmatized_words)\n",
    "    trigram_counts.update(get_trigrams(lemmatized_words))\n",
    "    \n",
    "    processed_sentences.append(\" \".join(lemmatized_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aa6295c-5ff1-41fa-a4d0-2af5f53ae5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary: {'generative': 254, 'ai': 4, 'transform': 580, 'create': 122, 'interact': 303, 'digital': 172, 'content': 107, 'generative ai': 255, 'ai transform': 20, 'transform create': 581, 'create interact': 123, 'interact digital': 304, 'digital content': 173, 'generative ai transform': 258, 'ai transform create': 21, 'transform create interact': 582, 'create interact digital': 124, 'interact digital content': 305, 'model': 390, 'learn': 328, 'pattern': 441, 'vast': 606, 'datasets': 145, 'produce': 461, 'original': 432, 'text': 560, 'image': 295, 'code': 93, 'model learn': 397, 'learn pattern': 329, 'pattern vast': 442, 'vast datasets': 607, 'datasets produce': 146, 'produce original': 462, 'original text': 433, 'text image': 561, 'image code': 298, 'model learn pattern': 398, 'learn pattern vast': 330, 'pattern vast datasets': 443, 'vast datasets produce': 608, 'datasets produce original': 147, 'produce original text': 463, 'original text image': 434, 'text image code': 563, 'large': 322, 'language': 319, 'like': 337, 'gpt': 267, 'write': 623, 'essay': 211, 'summarize': 540, 'document': 181, 'even': 218, 'debug': 149, 'software': 516, 'large language': 323, 'language model': 320, 'model like': 399, 'like gpt': 340, 'gpt write': 268, 'write essay': 624, 'essay summarize': 212, 'summarize document': 541, 'document even': 182, 'even debug': 219, 'debug software': 150, 'large language model': 324, 'language model like': 321, 'model like gpt': 401, 'like gpt write': 341, 'gpt write essay': 269, 'write essay summarize': 625, 'essay summarize document': 213, 'summarize document even': 542, 'document even debug': 183, 'even debug software': 220, 'diffusion': 169, 'generate': 248, 'photorealistic': 447, 'simple': 509, 'prompt': 464, 'diffusion model': 170, 'model generate': 393, 'generate photorealistic': 249, 'photorealistic image': 448, 'image simple': 299, 'simple text': 510, 'text prompt': 564, 'diffusion model generate': 171, 'model generate photorealistic': 394, 'generate photorealistic image': 250, 'photorealistic image simple': 449, 'image simple text': 300, 'simple text prompt': 511, 'one': 423, 'major': 358, 'challenge': 87, 'ensure': 202, 'output': 435, 'align': 25, 'human': 286, 'value': 605, 'one major': 424, 'major challenge': 359, 'challenge ensure': 88, 'ensure output': 203, 'output align': 436, 'align human': 26, 'human value': 287, 'one major challenge': 425, 'major challenge ensure': 360, 'challenge ensure output': 89, 'ensure output align': 204, 'output align human': 437, 'align human value': 27, 'bias': 65, 'train': 575, 'data': 140, 'often': 420, 'lead': 325, 'harmful': 282, 'mislead': 386, 'result': 498, 'bias train': 66, 'train data': 576, 'data often': 143, 'often lead': 421, 'lead harmful': 326, 'harmful mislead': 283, 'mislead result': 387, 'bias train data': 67, 'train data often': 577, 'data often lead': 144, 'often lead harmful': 422, 'lead harmful mislead': 327, 'harmful mislead result': 284, 'researchers': 495, 'use': 599, 'techniques': 551, 'rlhf': 500, 'refine': 477, 'behavior': 60, 'feedback': 235, 'researchers use': 496, 'use techniques': 603, 'techniques like': 554, 'like rlhf': 344, 'rlhf refine': 501, 'refine model': 478, 'model behavior': 391, 'behavior feedback': 61, 'researchers use techniques': 497, 'use techniques like': 604, 'techniques like rlhf': 555, 'like rlhf refine': 345, 'rlhf refine model': 502, 'refine model behavior': 479, 'model behavior feedback': 392, 'another': 31, 'issue': 311, 'computational': 100, 'costâ': 116, 'state': 532, 'of': 417, 'the': 565, 'art': 42, 'require': 492, 'millions': 378, 'dollars': 184, 'gpu': 270, 'hours': 285, 'another issue': 34, 'issue computational': 312, 'computational costâ': 101, 'costâ train': 117, 'train state': 578, 'state of': 533, 'of the': 418, 'the art': 566, 'art model': 45, 'model require': 402, 'require millions': 493, 'millions dollars': 379, 'dollars gpu': 185, 'gpu hours': 271, 'another issue computational': 35, 'issue computational costâ': 313, 'computational costâ train': 102, 'costâ train state': 118, 'train state of': 579, 'state of the': 534, 'of the art': 419, 'the art model': 567, 'art model require': 46, 'model require millions': 403, 'require millions dollars': 494, 'millions dollars gpu': 380, 'dollars gpu hours': 186, 'despite': 164, 'open': 426, 'source': 517, 'alternatives': 28, 'llama': 352, 'make': 361, 'technology': 556, 'accessible': 0, 'despite open': 165, 'open source': 427, 'source alternatives': 518, 'alternatives like': 29, 'like llama': 342, 'llama make': 353, 'make technology': 364, 'technology accessible': 557, 'despite open source': 166, 'open source alternatives': 428, 'source alternatives like': 519, 'alternatives like llama': 30, 'like llama make': 343, 'llama make technology': 354, 'make technology accessible': 365, 'startups': 529, 'leverage': 334, 'market': 366, 'copy': 110, 'video': 609, 'game': 242, 'assets': 49, 'personalize': 444, 'education': 190, 'tool': 574, 'startups leverage': 530, 'leverage generative': 335, 'ai market': 17, 'market copy': 367, 'copy video': 111, 'video game': 610, 'game assets': 243, 'assets personalize': 50, 'personalize education': 445, 'education tool': 191, 'startups leverage generative': 531, 'leverage generative ai': 336, 'generative ai market': 257, 'ai market copy': 18, 'market copy video': 368, 'copy video game': 112, 'video game assets': 611, 'game assets personalize': 244, 'assets personalize education': 51, 'personalize education tool': 446, 'critics': 134, 'argue': 39, 'systems': 546, 'lack': 316, 'true': 587, 'understand': 590, 'merely': 375, 'remix': 486, 'exist': 221, 'critics argue': 135, 'argue systems': 40, 'systems lack': 549, 'lack true': 317, 'true understand': 588, 'understand merely': 591, 'merely remix': 376, 'remix exist': 487, 'exist content': 222, 'critics argue systems': 136, 'argue systems lack': 41, 'systems lack true': 550, 'lack true understand': 318, 'true understand merely': 589, 'understand merely remix': 592, 'merely remix exist': 377, 'remix exist content': 488, 'copyright': 113, 'dispute': 175, 'erupt': 208, 'generated': 251, 'mimic': 381, 'live': 349, 'artistsâ': 47, 'style': 538, 'copyright dispute': 114, 'dispute erupt': 176, 'erupt ai': 209, 'ai generated': 13, 'generated art': 252, 'art mimic': 43, 'mimic live': 382, 'live artistsâ': 350, 'artistsâ style': 48, 'copyright dispute erupt': 115, 'dispute erupt ai': 177, 'erupt ai generated': 210, 'ai generated art': 14, 'generated art mimic': 253, 'art mimic live': 44, 'mimic live artistsâ': 383, 'live artistsâ style': 351, 'meanwhile': 369, 'enterprises': 205, 'deploy': 160, 'chatbots': 90, 'handle': 279, 'customer': 137, 'service': 503, 'mix': 388, 'success': 539, 'meanwhile enterprises': 370, 'enterprises deploy': 206, 'deploy chatbots': 161, 'chatbots handle': 91, 'handle customer': 280, 'customer service': 138, 'service mix': 504, 'mix success': 389, 'meanwhile enterprises deploy': 371, 'enterprises deploy chatbots': 207, 'deploy chatbots handle': 162, 'chatbots handle customer': 92, 'handle customer service': 281, 'customer service mix': 139, 'service mix success': 505, 'hallucinationsâ': 276, 'invent': 308, 'false': 229, 'factsâ': 226, 'remain': 483, 'critical': 131, 'flaw': 236, 'medical': 372, 'legal': 331, 'case': 83, 'hallucinationsâ model': 277, 'model invent': 395, 'invent false': 309, 'false factsâ': 230, 'factsâ remain': 227, 'remain critical': 484, 'critical flaw': 132, 'flaw medical': 237, 'medical legal': 373, 'legal use': 332, 'use case': 602, 'hallucinationsâ model invent': 278, 'model invent false': 396, 'invent false factsâ': 310, 'false factsâ remain': 231, 'factsâ remain critical': 228, 'remain critical flaw': 485, 'critical flaw medical': 133, 'flaw medical legal': 238, 'medical legal use': 374, 'legal use case': 333, 'governments': 264, 'draft': 187, 'regulations': 480, 'address': 1, 'deepfakes': 151, 'misinformation': 384, 'risk': 499, 'governments draft': 265, 'draft regulations': 188, 'regulations address': 481, 'address deepfakes': 2, 'deepfakes misinformation': 152, 'misinformation risk': 385, 'governments draft regulations': 266, 'draft regulations address': 189, 'regulations address deepfakes': 482, 'address deepfakes misinformation': 3, 'deepfakes misinformation risk': 153, 'creative': 125, 'side': 506, 'writers': 626, 'brainstorm': 74, 'rarely': 468, 'publish': 465, 'raw': 471, 'creative side': 126, 'side writers': 507, 'writers use': 627, 'use ai': 600, 'ai brainstorm': 7, 'brainstorm rarely': 75, 'rarely publish': 469, 'publish raw': 466, 'raw output': 472, 'creative side writers': 127, 'side writers use': 508, 'writers use ai': 628, 'use ai brainstorm': 601, 'ai brainstorm rarely': 8, 'brainstorm rarely publish': 76, 'rarely publish raw': 470, 'publish raw output': 467, 'line': 346, 'assistance': 52, 'replacement': 489, 'spark': 520, 'ethical': 216, 'debate': 148, 'line assistance': 347, 'assistance replacement': 53, 'replacement spark': 490, 'spark ethical': 521, 'ethical debate': 217, 'line assistance replacement': 348, 'assistance replacement spark': 54, 'replacement spark ethical': 491, 'spark ethical debate': 522, 'educators': 192, 'worry': 620, 'students': 535, 'outsource': 438, 'undetectable': 593, 'educators worry': 193, 'worry students': 621, 'students outsource': 536, 'outsource essay': 439, 'essay undetectable': 214, 'undetectable ai': 594, 'ai tool': 19, 'educators worry students': 194, 'worry students outsource': 622, 'students outsource essay': 537, 'outsource essay undetectable': 440, 'essay undetectable ai': 215, 'undetectable ai tool': 595, 'company': 97, 'watermark': 612, 'distinguish': 178, 'work': 618, 'company watermark': 98, 'watermark ai': 613, 'ai content': 9, 'content distinguish': 108, 'distinguish human': 179, 'human work': 288, 'company watermark ai': 99, 'watermark ai content': 614, 'ai content distinguish': 10, 'content distinguish human': 109, 'distinguish human work': 180, 'hybrid': 292, 'combine': 94, 'symbolic': 543, 'logic': 355, 'neural': 411, 'network': 408, 'aim': 22, 'improve': 301, 'reason': 476, 'hybrid systems': 293, 'systems combine': 547, 'combine symbolic': 95, 'symbolic logic': 544, 'logic neural': 356, 'neural network': 412, 'network aim': 409, 'aim improve': 23, 'improve reason': 302, 'hybrid systems combine': 294, 'systems combine symbolic': 548, 'combine symbolic logic': 96, 'symbolic logic neural': 545, 'logic neural network': 357, 'neural network aim': 413, 'network aim improve': 410, 'aim improve reason': 24, 'multimodal': 405, 'gemini': 245, 'process': 458, 'audio': 58, 'simultaneously': 512, 'multimodal model': 406, 'like gemini': 338, 'gemini process': 246, 'process text': 459, 'image audio': 296, 'audio simultaneously': 59, 'multimodal model like': 407, 'model like gemini': 400, 'like gemini process': 339, 'gemini process text': 247, 'process text image': 460, 'text image audio': 562, 'image audio simultaneously': 297, 'real': 473, 'time': 568, 'translation': 583, 'apps': 36, 'preserve': 455, 'speaker': 523, 'tone': 572, 'emotion': 198, 'real time': 474, 'time translation': 569, 'translation apps': 584, 'apps preserve': 37, 'preserve speaker': 456, 'speaker tone': 524, 'tone emotion': 573, 'real time translation': 475, 'time translation apps': 570, 'translation apps preserve': 585, 'apps preserve speaker': 38, 'preserve speaker tone': 457, 'speaker tone emotion': 525, 'energy': 199, 'consumption': 104, 'center': 84, 'power': 452, 'grow': 272, 'concern': 103, 'energy consumption': 200, 'consumption data': 105, 'data center': 141, 'center power': 85, 'power ai': 453, 'ai another': 5, 'another grow': 32, 'grow concern': 275, 'energy consumption data': 201, 'consumption data center': 106, 'data center power': 142, 'center power ai': 86, 'power ai another': 454, 'ai another grow': 6, 'another grow concern': 33, 'optimists': 429, 'believe': 62, 'could': 119, 'democratize': 154, 'creativity': 128, 'skeptics': 513, 'fear': 232, 'job': 314, 'displacement': 174, 'optimists believe': 430, 'believe generative': 63, 'ai could': 11, 'could democratize': 120, 'democratize creativity': 155, 'creativity skeptics': 129, 'skeptics fear': 514, 'fear job': 233, 'job displacement': 315, 'optimists believe generative': 431, 'believe generative ai': 64, 'generative ai could': 256, 'ai could democratize': 12, 'could democratize creativity': 121, 'democratize creativity skeptics': 156, 'creativity skeptics fear': 130, 'skeptics fear job': 515, 'fear job displacement': 234, 'explainability': 223, 'attempt': 55, 'black': 68, 'box': 71, 'transparent': 586, 'explainability techniques': 224, 'techniques attempt': 552, 'attempt make': 56, 'make black': 362, 'black box': 69, 'box model': 72, 'model transparent': 404, 'explainability techniques attempt': 225, 'techniques attempt make': 553, 'attempt make black': 57, 'make black box': 363, 'black box model': 70, 'box model transparent': 73, 'capabilities': 80, 'call': 77, 'global': 259, 'standards': 526, 'development': 167, 'deployment': 163, 'capabilities grow': 81, 'grow call': 273, 'call global': 78, 'global standards': 260, 'standards development': 527, 'development deployment': 168, 'capabilities grow call': 82, 'grow call global': 274, 'call global standards': 79, 'global standards development': 261, 'standards development deployment': 528, 'next': 414, 'frontier': 239, 'embody': 195, 'physical': 450, 'world': 619, 'next frontier': 415, 'frontier embody': 240, 'embody ai': 196, 'ai interact': 15, 'interact physical': 306, 'physical world': 451, 'next frontier embody': 416, 'frontier embody ai': 241, 'embody ai interact': 197, 'ai interact physical': 16, 'interact physical world': 307, 'whether': 615, 'uplift': 596, 'humanity': 289, 'depend': 157, 'govern': 262, 'today': 571, 'whether technology': 616, 'technology uplift': 558, 'uplift humanity': 597, 'humanity depend': 290, 'depend govern': 158, 'govern today': 263, 'whether technology uplift': 617, 'technology uplift humanity': 559, 'uplift humanity depend': 598, 'humanity depend govern': 291, 'depend govern today': 159}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "vectorizer.fit(processed_sentences)\n",
    "print(\"\\nVocabulary:\", vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "807f48ff-e7d3-44e9-9ac3-1bee6ec1ee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Sentences: ['Generative AI transform create interact digital content .', 'model learn pattern vast datasets produce original text , image , code .', 'Large language model like GPT-4 write essay , summarize document , even debug software .', 'Diffusion model generate photorealistic image simple text prompt .', 'One major challenge ensure output align human value .', 'Bias train data often lead harmful mislead result .', 'Researchers use techniques like RLHF refine model behavior feedback .', 'Another issue computational costâ€ ” train state-of-the-art model require millions dollars GPU hours .', 'Despite , open-source alternatives like LLaMA make technology accessible .', 'Startups leverage generative AI market copy , video game assets , personalize education tool .', 'Critics argue systems lack true understand merely remix exist content .', 'Copyright dispute erupt AI-generated art mimic live artistsâ€™ style .', 'Meanwhile , enterprises deploy chatbots handle customer service mix success .', 'Hallucinationsâ€ ” model invent false factsâ€ ” remain critical flaw medical legal use case .', 'Governments draft regulations address deepfakes misinformation risk .', 'creative side , writers use AI brainstorm rarely publish raw output .', 'line assistance replacement spark ethical debate .', 'Educators worry students outsource essay undetectable AI tool .', 'company watermark AI content distinguish human work .', 'Hybrid systems combine symbolic logic neural network aim improve reason .', 'Multimodal model like Gemini process text , image , audio simultaneously .', 'Real-time translation apps preserve speaker tone emotion .', 'Energy consumption data center power AI another grow concern .', 'Optimists believe generative AI could democratize creativity , skeptics fear job displacement .', 'Explainability techniques attempt make black-box model transparent .', 'capabilities grow , call global standards development deployment .', 'next frontier embody AI interact physical world .', 'Whether technology uplift humanity depend govern today .']\n",
      "\n",
      "Trigram Probabilities:\n",
      "P(transform | Generative, AI) = 1.00\n",
      "P(create | AI, transform) = 0.12\n",
      "P(interact | transform, create) = 1.00\n",
      "P(digital | create, interact) = 1.00\n",
      "P(content | interact, digital) = 0.50\n",
      "P(. | digital, content) = 1.00\n",
      "P(pattern | model, learn) = 0.12\n",
      "P(vast | learn, pattern) = 1.00\n",
      "P(datasets | pattern, vast) = 1.00\n",
      "P(produce | vast, datasets) = 1.00\n",
      "P(original | datasets, produce) = 1.00\n",
      "P(text | produce, original) = 1.00\n",
      "P(, | original, text) = 1.00\n",
      "P(image | text, ,) = 0.67\n",
      "P(, | ,, image) = 0.15\n",
      "P(code | image, ,) = 0.33\n",
      "P(. | ,, code) = 0.08\n",
      "P(model | Large, language) = 1.00\n",
      "P(like | language, model) = 1.00\n",
      "P(GPT-4 | model, like) = 0.12\n",
      "P(write | like, GPT-4) = 0.25\n",
      "P(essay | GPT-4, write) = 1.00\n",
      "P(, | write, essay) = 1.00\n",
      "P(summarize | essay, ,) = 0.50\n",
      "P(document | ,, summarize) = 0.08\n",
      "P(, | summarize, document) = 1.00\n",
      "P(even | document, ,) = 1.00\n",
      "P(debug | ,, even) = 0.08\n",
      "P(software | even, debug) = 1.00\n",
      "P(. | debug, software) = 1.00\n",
      "P(generate | Diffusion, model) = 1.00\n",
      "P(photorealistic | model, generate) = 0.12\n",
      "P(image | generate, photorealistic) = 1.00\n",
      "P(simple | photorealistic, image) = 1.00\n",
      "P(text | image, simple) = 0.33\n",
      "P(prompt | simple, text) = 1.00\n",
      "P(. | text, prompt) = 0.33\n",
      "P(challenge | One, major) = 1.00\n",
      "P(ensure | major, challenge) = 1.00\n",
      "P(output | challenge, ensure) = 1.00\n",
      "P(align | ensure, output) = 1.00\n",
      "P(human | output, align) = 0.50\n",
      "P(value | align, human) = 1.00\n",
      "P(. | human, value) = 0.50\n",
      "P(data | Bias, train) = 1.00\n",
      "P(often | train, data) = 0.50\n",
      "P(lead | data, often) = 0.50\n",
      "P(harmful | often, lead) = 1.00\n",
      "P(mislead | lead, harmful) = 1.00\n",
      "P(result | harmful, mislead) = 1.00\n",
      "P(. | mislead, result) = 1.00\n",
      "P(techniques | Researchers, use) = 1.00\n",
      "P(like | use, techniques) = 0.33\n",
      "P(RLHF | techniques, like) = 0.50\n",
      "P(refine | like, RLHF) = 0.25\n",
      "P(model | RLHF, refine) = 1.00\n",
      "P(behavior | refine, model) = 1.00\n",
      "P(feedback | model, behavior) = 0.12\n",
      "P(. | behavior, feedback) = 1.00\n",
      "P(computational | Another, issue) = 1.00\n",
      "P(costâ€ | issue, computational) = 1.00\n",
      "P(” | computational, costâ€) = 1.00\n",
      "P(train | costâ€, ”) = 1.00\n",
      "P(state-of-the-art | ”, train) = 0.33\n",
      "P(model | train, state-of-the-art) = 0.50\n",
      "P(require | state-of-the-art, model) = 1.00\n",
      "P(millions | model, require) = 0.12\n",
      "P(dollars | require, millions) = 1.00\n",
      "P(GPU | millions, dollars) = 1.00\n",
      "P(hours | dollars, GPU) = 1.00\n",
      "P(. | GPU, hours) = 1.00\n",
      "P(open-source | Despite, ,) = 1.00\n",
      "P(alternatives | ,, open-source) = 0.08\n",
      "P(like | open-source, alternatives) = 1.00\n",
      "P(LLaMA | alternatives, like) = 1.00\n",
      "P(make | like, LLaMA) = 0.25\n",
      "P(technology | LLaMA, make) = 1.00\n",
      "P(accessible | make, technology) = 0.50\n",
      "P(. | technology, accessible) = 0.50\n",
      "P(generative | Startups, leverage) = 1.00\n",
      "P(AI | leverage, generative) = 1.00\n",
      "P(market | generative, AI) = 0.50\n",
      "P(copy | AI, market) = 0.12\n",
      "P(, | market, copy) = 1.00\n",
      "P(video | copy, ,) = 1.00\n",
      "P(game | ,, video) = 0.08\n",
      "P(assets | video, game) = 1.00\n",
      "P(, | game, assets) = 1.00\n",
      "P(personalize | assets, ,) = 1.00\n",
      "P(education | ,, personalize) = 0.08\n",
      "P(tool | personalize, education) = 1.00\n",
      "P(. | education, tool) = 1.00\n",
      "P(systems | Critics, argue) = 1.00\n",
      "P(lack | argue, systems) = 1.00\n",
      "P(true | systems, lack) = 0.50\n",
      "P(understand | lack, true) = 1.00\n",
      "P(merely | true, understand) = 1.00\n",
      "P(remix | understand, merely) = 1.00\n",
      "P(exist | merely, remix) = 1.00\n",
      "P(content | remix, exist) = 1.00\n",
      "P(. | exist, content) = 1.00\n",
      "P(erupt | Copyright, dispute) = 1.00\n",
      "P(AI-generated | dispute, erupt) = 1.00\n",
      "P(art | erupt, AI-generated) = 1.00\n",
      "P(mimic | AI-generated, art) = 1.00\n",
      "P(live | art, mimic) = 1.00\n",
      "P(artistsâ€™ | mimic, live) = 1.00\n",
      "P(style | live, artistsâ€™) = 1.00\n",
      "P(. | artistsâ€™, style) = 1.00\n",
      "P(enterprises | Meanwhile, ,) = 1.00\n",
      "P(deploy | ,, enterprises) = 0.08\n",
      "P(chatbots | enterprises, deploy) = 1.00\n",
      "P(handle | deploy, chatbots) = 1.00\n",
      "P(customer | chatbots, handle) = 1.00\n",
      "P(service | handle, customer) = 1.00\n",
      "P(mix | customer, service) = 1.00\n",
      "P(success | service, mix) = 1.00\n",
      "P(. | mix, success) = 1.00\n",
      "P(model | Hallucinationsâ€, ”) = 1.00\n",
      "P(invent | ”, model) = 0.33\n",
      "P(false | model, invent) = 0.12\n",
      "P(factsâ€ | invent, false) = 1.00\n",
      "P(” | false, factsâ€) = 1.00\n",
      "P(remain | factsâ€, ”) = 1.00\n",
      "P(critical | ”, remain) = 0.33\n",
      "P(flaw | remain, critical) = 1.00\n",
      "P(medical | critical, flaw) = 1.00\n",
      "P(legal | flaw, medical) = 1.00\n",
      "P(use | medical, legal) = 1.00\n",
      "P(case | legal, use) = 1.00\n",
      "P(. | use, case) = 0.33\n",
      "P(regulations | Governments, draft) = 1.00\n",
      "P(address | draft, regulations) = 1.00\n",
      "P(deepfakes | regulations, address) = 1.00\n",
      "P(misinformation | address, deepfakes) = 1.00\n",
      "P(risk | deepfakes, misinformation) = 1.00\n",
      "P(. | misinformation, risk) = 1.00\n",
      "P(, | creative, side) = 1.00\n",
      "P(writers | side, ,) = 1.00\n",
      "P(use | ,, writers) = 0.08\n",
      "P(AI | writers, use) = 1.00\n",
      "P(brainstorm | use, AI) = 0.33\n",
      "P(rarely | AI, brainstorm) = 0.12\n",
      "P(publish | brainstorm, rarely) = 1.00\n",
      "P(raw | rarely, publish) = 1.00\n",
      "P(output | publish, raw) = 1.00\n",
      "P(. | raw, output) = 1.00\n",
      "P(replacement | line, assistance) = 1.00\n",
      "P(spark | assistance, replacement) = 1.00\n",
      "P(ethical | replacement, spark) = 1.00\n",
      "P(debate | spark, ethical) = 1.00\n",
      "P(. | ethical, debate) = 1.00\n",
      "P(students | Educators, worry) = 1.00\n",
      "P(outsource | worry, students) = 1.00\n",
      "P(essay | students, outsource) = 1.00\n",
      "P(undetectable | outsource, essay) = 1.00\n",
      "P(AI | essay, undetectable) = 0.50\n",
      "P(tool | undetectable, AI) = 1.00\n",
      "P(. | AI, tool) = 0.12\n",
      "P(AI | company, watermark) = 1.00\n",
      "P(content | watermark, AI) = 1.00\n",
      "P(distinguish | AI, content) = 0.12\n",
      "P(human | content, distinguish) = 0.33\n",
      "P(work | distinguish, human) = 1.00\n",
      "P(. | human, work) = 0.50\n",
      "P(combine | Hybrid, systems) = 1.00\n",
      "P(symbolic | systems, combine) = 0.50\n",
      "P(logic | combine, symbolic) = 1.00\n",
      "P(neural | symbolic, logic) = 1.00\n",
      "P(network | logic, neural) = 1.00\n",
      "P(aim | neural, network) = 1.00\n",
      "P(improve | network, aim) = 1.00\n",
      "P(reason | aim, improve) = 1.00\n",
      "P(. | improve, reason) = 1.00\n",
      "P(like | Multimodal, model) = 1.00\n",
      "P(Gemini | model, like) = 0.12\n",
      "P(process | like, Gemini) = 0.25\n",
      "P(text | Gemini, process) = 1.00\n",
      "P(, | process, text) = 1.00\n",
      "P(audio | image, ,) = 0.33\n",
      "P(simultaneously | ,, audio) = 0.08\n",
      "P(. | audio, simultaneously) = 1.00\n",
      "P(apps | Real-time, translation) = 1.00\n",
      "P(preserve | translation, apps) = 1.00\n",
      "P(speaker | apps, preserve) = 1.00\n",
      "P(tone | preserve, speaker) = 1.00\n",
      "P(emotion | speaker, tone) = 1.00\n",
      "P(. | tone, emotion) = 1.00\n",
      "P(data | Energy, consumption) = 1.00\n",
      "P(center | consumption, data) = 1.00\n",
      "P(power | data, center) = 0.50\n",
      "P(AI | center, power) = 1.00\n",
      "P(another | power, AI) = 1.00\n",
      "P(grow | AI, another) = 0.12\n",
      "P(concern | another, grow) = 1.00\n",
      "P(. | grow, concern) = 0.50\n",
      "P(generative | Optimists, believe) = 1.00\n",
      "P(AI | believe, generative) = 1.00\n",
      "P(could | generative, AI) = 0.50\n",
      "P(democratize | AI, could) = 0.12\n",
      "P(creativity | could, democratize) = 1.00\n",
      "P(, | democratize, creativity) = 1.00\n",
      "P(skeptics | creativity, ,) = 1.00\n",
      "P(fear | ,, skeptics) = 0.08\n",
      "P(job | skeptics, fear) = 1.00\n",
      "P(displacement | fear, job) = 1.00\n",
      "P(. | job, displacement) = 1.00\n",
      "P(attempt | Explainability, techniques) = 1.00\n",
      "P(make | techniques, attempt) = 0.50\n",
      "P(black-box | attempt, make) = 1.00\n",
      "P(model | make, black-box) = 0.50\n",
      "P(transparent | black-box, model) = 1.00\n",
      "P(. | model, transparent) = 0.12\n",
      "P(, | capabilities, grow) = 1.00\n",
      "P(call | grow, ,) = 0.50\n",
      "P(global | ,, call) = 0.08\n",
      "P(standards | call, global) = 1.00\n",
      "P(development | global, standards) = 1.00\n",
      "P(deployment | standards, development) = 1.00\n",
      "P(. | development, deployment) = 1.00\n",
      "P(embody | next, frontier) = 1.00\n",
      "P(AI | frontier, embody) = 1.00\n",
      "P(interact | embody, AI) = 1.00\n",
      "P(physical | AI, interact) = 0.12\n",
      "P(world | interact, physical) = 0.50\n",
      "P(. | physical, world) = 1.00\n",
      "P(uplift | Whether, technology) = 1.00\n",
      "P(humanity | technology, uplift) = 0.50\n",
      "P(depend | uplift, humanity) = 1.00\n",
      "P(govern | humanity, depend) = 1.00\n",
      "P(today | depend, govern) = 1.00\n",
      "P(. | govern, today) = 1.00\n"
     ]
    }
   ],
   "source": [
    "trigram_probabilities = {\n",
    "    trigram: count / word_counts[trigram[0]]\n",
    "    for trigram, count in trigram_counts.items() if trigram[0] in word_counts\n",
    "}\n",
    "\n",
    "print(\"\\nProcessed Sentences:\", processed_sentences)\n",
    "print(\"\\nTrigram Probabilities:\")\n",
    "for trigram, prob in trigram_probabilities.items():\n",
    "    print(f\"P({trigram[2]} | {trigram[0]}, {trigram[1]}) = {prob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f452d-8800-4120-beac-be1d3623dd29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
