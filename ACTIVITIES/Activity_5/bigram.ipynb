{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c3492d-8fc4-4cf0-ae8d-de515e81479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter \n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de699c66-0f9e-4cc3-9dbd-95f9debfaaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=open(\"C:/Users/Lenovo/Downloads/GenAl/Activity_5/corpus.txt\").read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b17c0ac-26d4-44ad-a245-df2b643e6794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7e97e69-45ca-4475-aaaa-c72a9986bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d716b7-8c44-4c0a-a87f-498537835559",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts=Counter()\n",
    "word_counts=Counter()\n",
    "processed_sentences=[]\n",
    "\n",
    "def get_bigrams(words):\n",
    "    return list(zip(words,islice(words,1,None)))\n",
    "\n",
    "for sentence in corpus:\n",
    "    words =word_tokenize(sentence)\n",
    "    filtered_words=[word for word in words if word.lower() not in stop_words]\n",
    "    lemmatized_words=[lemmatizer.lemmatize(word,pos='v')for word in filtered_words]\n",
    "    word_counts.update(lemmatized_words)\n",
    "    bigram_counts.update(get_bigrams(lemmatized_words))\n",
    "    processed_sentences.append(\" \".join(lemmatized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75d25951-e1d2-48a0-a909-e131b31e3f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary: {'generative': 175, 'ai': 3, 'transform': 398, 'create': 82, 'interact': 210, 'digital': 117, 'content': 72, 'generative ai': 176, 'ai transform': 12, 'transform create': 399, 'create interact': 83, 'interact digital': 211, 'digital content': 118, 'model': 269, 'learn': 227, 'pattern': 301, 'vast': 416, 'datasets': 97, 'produce': 315, 'original': 295, 'text': 384, 'image': 204, 'code': 62, 'model learn': 273, 'learn pattern': 228, 'pattern vast': 302, 'vast datasets': 417, 'datasets produce': 98, 'produce original': 316, 'original text': 296, 'text image': 385, 'image code': 206, 'large': 223, 'language': 221, 'like': 233, 'gpt': 183, 'write': 428, 'essay': 145, 'summarize': 371, 'document': 124, 'even': 150, 'debug': 100, 'software': 354, 'large language': 224, 'language model': 222, 'model like': 274, 'like gpt': 235, 'gpt write': 184, 'write essay': 429, 'essay summarize': 146, 'summarize document': 372, 'document even': 125, 'even debug': 151, 'debug software': 101, 'diffusion': 115, 'generate': 171, 'photorealistic': 305, 'simple': 349, 'prompt': 317, 'diffusion model': 116, 'model generate': 271, 'generate photorealistic': 172, 'photorealistic image': 306, 'image simple': 207, 'simple text': 350, 'text prompt': 386, 'one': 289, 'major': 246, 'challenge': 58, 'ensure': 139, 'output': 297, 'align': 15, 'human': 197, 'value': 415, 'one major': 290, 'major challenge': 247, 'challenge ensure': 59, 'ensure output': 140, 'output align': 298, 'align human': 16, 'human value': 198, 'bias': 43, 'train': 395, 'data': 94, 'often': 287, 'lead': 225, 'harmful': 194, 'mislead': 265, 'result': 341, 'bias train': 44, 'train data': 396, 'data often': 96, 'often lead': 288, 'lead harmful': 226, 'harmful mislead': 195, 'mislead result': 266, 'researchers': 339, 'use': 411, 'techniques': 378, 'rlhf': 343, 'refine': 327, 'behavior': 39, 'feedback': 162, 'researchers use': 340, 'use techniques': 414, 'techniques like': 380, 'like rlhf': 237, 'rlhf refine': 344, 'refine model': 328, 'model behavior': 270, 'behavior feedback': 40, 'another': 19, 'issue': 215, 'computational': 67, 'costâ': 78, 'state': 365, 'of': 285, 'the': 387, 'art': 26, 'require': 337, 'millions': 259, 'dollars': 126, 'gpu': 185, 'hours': 196, 'another issue': 21, 'issue computational': 216, 'computational costâ': 68, 'costâ train': 79, 'train state': 397, 'state of': 366, 'of the': 286, 'the art': 388, 'art model': 28, 'model require': 275, 'require millions': 338, 'millions dollars': 260, 'dollars gpu': 127, 'gpu hours': 186, 'despite': 111, 'open': 291, 'source': 355, 'alternatives': 17, 'llama': 242, 'make': 248, 'technology': 381, 'accessible': 0, 'despite open': 112, 'open source': 292, 'source alternatives': 356, 'alternatives like': 18, 'like llama': 236, 'llama make': 243, 'make technology': 250, 'technology accessible': 382, 'startups': 363, 'leverage': 231, 'market': 251, 'copy': 74, 'video': 418, 'game': 167, 'assets': 31, 'personalize': 303, 'education': 130, 'tool': 394, 'startups leverage': 364, 'leverage generative': 232, 'ai market': 10, 'market copy': 252, 'copy video': 75, 'video game': 419, 'game assets': 168, 'assets personalize': 32, 'personalize education': 304, 'education tool': 131, 'critics': 90, 'argue': 24, 'systems': 375, 'lack': 219, 'true': 403, 'understand': 405, 'merely': 257, 'remix': 333, 'exist': 152, 'critics argue': 91, 'argue systems': 25, 'systems lack': 377, 'lack true': 220, 'true understand': 404, 'understand merely': 406, 'merely remix': 258, 'remix exist': 334, 'exist content': 153, 'copyright': 76, 'dispute': 120, 'erupt': 143, 'generated': 173, 'mimic': 261, 'live': 240, 'artistsâ': 29, 'style': 369, 'copyright dispute': 77, 'dispute erupt': 121, 'erupt ai': 144, 'ai generated': 8, 'generated art': 174, 'art mimic': 27, 'mimic live': 262, 'live artistsâ': 241, 'artistsâ style': 30, 'meanwhile': 253, 'enterprises': 141, 'deploy': 108, 'chatbots': 60, 'handle': 192, 'customer': 92, 'service': 345, 'mix': 267, 'success': 370, 'meanwhile enterprises': 254, 'enterprises deploy': 142, 'deploy chatbots': 109, 'chatbots handle': 61, 'handle customer': 193, 'customer service': 93, 'service mix': 346, 'mix success': 268, 'hallucinationsâ': 190, 'invent': 213, 'false': 158, 'factsâ': 156, 'remain': 331, 'critical': 88, 'flaw': 163, 'medical': 255, 'legal': 229, 'case': 55, 'hallucinationsâ model': 191, 'model invent': 272, 'invent false': 214, 'false factsâ': 159, 'factsâ remain': 157, 'remain critical': 332, 'critical flaw': 89, 'flaw medical': 164, 'medical legal': 256, 'legal use': 230, 'use case': 413, 'governments': 181, 'draft': 128, 'regulations': 329, 'address': 1, 'deepfakes': 102, 'misinformation': 263, 'risk': 342, 'governments draft': 182, 'draft regulations': 129, 'regulations address': 330, 'address deepfakes': 2, 'deepfakes misinformation': 103, 'misinformation risk': 264, 'creative': 84, 'side': 347, 'writers': 430, 'brainstorm': 49, 'rarely': 320, 'publish': 318, 'raw': 322, 'creative side': 85, 'side writers': 348, 'writers use': 431, 'use ai': 412, 'ai brainstorm': 5, 'brainstorm rarely': 50, 'rarely publish': 321, 'publish raw': 319, 'raw output': 323, 'line': 238, 'assistance': 33, 'replacement': 335, 'spark': 357, 'ethical': 148, 'debate': 99, 'line assistance': 239, 'assistance replacement': 34, 'replacement spark': 336, 'spark ethical': 358, 'ethical debate': 149, 'educators': 132, 'worry': 426, 'students': 367, 'outsource': 299, 'undetectable': 407, 'educators worry': 133, 'worry students': 427, 'students outsource': 368, 'outsource essay': 300, 'essay undetectable': 147, 'undetectable ai': 408, 'ai tool': 11, 'company': 65, 'watermark': 420, 'distinguish': 122, 'work': 424, 'company watermark': 66, 'watermark ai': 421, 'ai content': 6, 'content distinguish': 73, 'distinguish human': 123, 'human work': 199, 'hybrid': 202, 'combine': 63, 'symbolic': 373, 'logic': 244, 'neural': 281, 'network': 279, 'aim': 13, 'improve': 208, 'reason': 326, 'hybrid systems': 203, 'systems combine': 376, 'combine symbolic': 64, 'symbolic logic': 374, 'logic neural': 245, 'neural network': 282, 'network aim': 280, 'aim improve': 14, 'improve reason': 209, 'multimodal': 277, 'gemini': 169, 'process': 313, 'audio': 37, 'simultaneously': 351, 'multimodal model': 278, 'like gemini': 234, 'gemini process': 170, 'process text': 314, 'image audio': 205, 'audio simultaneously': 38, 'real': 324, 'time': 389, 'translation': 400, 'apps': 22, 'preserve': 311, 'speaker': 359, 'tone': 392, 'emotion': 136, 'real time': 325, 'time translation': 390, 'translation apps': 401, 'apps preserve': 23, 'preserve speaker': 312, 'speaker tone': 360, 'tone emotion': 393, 'energy': 137, 'consumption': 70, 'center': 56, 'power': 309, 'grow': 187, 'concern': 69, 'energy consumption': 138, 'consumption data': 71, 'data center': 95, 'center power': 57, 'power ai': 310, 'ai another': 4, 'another grow': 20, 'grow concern': 189, 'optimists': 293, 'believe': 41, 'could': 80, 'democratize': 104, 'creativity': 86, 'skeptics': 352, 'fear': 160, 'job': 217, 'displacement': 119, 'optimists believe': 294, 'believe generative': 42, 'ai could': 7, 'could democratize': 81, 'democratize creativity': 105, 'creativity skeptics': 87, 'skeptics fear': 353, 'fear job': 161, 'job displacement': 218, 'explainability': 154, 'attempt': 35, 'black': 45, 'box': 47, 'transparent': 402, 'explainability techniques': 155, 'techniques attempt': 379, 'attempt make': 36, 'make black': 249, 'black box': 46, 'box model': 48, 'model transparent': 276, 'capabilities': 53, 'call': 51, 'global': 177, 'standards': 361, 'development': 113, 'deployment': 110, 'capabilities grow': 54, 'grow call': 188, 'call global': 52, 'global standards': 178, 'standards development': 362, 'development deployment': 114, 'next': 283, 'frontier': 165, 'embody': 134, 'physical': 307, 'world': 425, 'next frontier': 284, 'frontier embody': 166, 'embody ai': 135, 'ai interact': 9, 'interact physical': 212, 'physical world': 308, 'whether': 422, 'uplift': 409, 'humanity': 200, 'depend': 106, 'govern': 179, 'today': 391, 'whether technology': 423, 'technology uplift': 383, 'uplift humanity': 410, 'humanity depend': 201, 'depend govern': 107, 'govern today': 180}\n"
     ]
    }
   ],
   "source": [
    "v=CountVectorizer(ngram_range=(1,2))\n",
    "v.fit(processed_sentences)\n",
    "print(\"\\nVocabulary:\",v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44463f3d-c151-4ad5-8b91-abfae83bc19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Sentences: ['Generative AI transform create interact digital content .', 'model learn pattern vast datasets produce original text , image , code .', 'Large language model like GPT-4 write essay , summarize document , even debug software .', 'Diffusion model generate photorealistic image simple text prompt .', 'One major challenge ensure output align human value .', 'Bias train data often lead harmful mislead result .', 'Researchers use techniques like RLHF refine model behavior feedback .', 'Another issue computational costâ€ ” train state-of-the-art model require millions dollars GPU hours .', 'Despite , open-source alternatives like LLaMA make technology accessible .', 'Startups leverage generative AI market copy , video game assets , personalize education tool .', 'Critics argue systems lack true understand merely remix exist content .', 'Copyright dispute erupt AI-generated art mimic live artistsâ€™ style .', 'Meanwhile , enterprises deploy chatbots handle customer service mix success .', 'Hallucinationsâ€ ” model invent false factsâ€ ” remain critical flaw medical legal use case .', 'Governments draft regulations address deepfakes misinformation risk .', 'creative side , writers use AI brainstorm rarely publish raw output .', 'line assistance replacement spark ethical debate .', 'Educators worry students outsource essay undetectable AI tool .', 'company watermark AI content distinguish human work .', 'Hybrid systems combine symbolic logic neural network aim improve reason .', 'Multimodal model like Gemini process text , image , audio simultaneously .', 'Real-time translation apps preserve speaker tone emotion .', 'Energy consumption data center power AI another grow concern .', 'Optimists believe generative AI could democratize creativity , skeptics fear job displacement .', 'Explainability techniques attempt make black-box model transparent .', 'capabilities grow , call global standards development deployment .', 'next frontier embody AI interact physical world .', 'Whether technology uplift humanity depend govern today .']\n",
      "\n",
      "Bigram Probailites:\n",
      "P(AI | Generative)=1.00\n",
      "P(transform | AI)=0.12\n",
      "P(create | transform)=1.00\n",
      "P(interact | create)=1.00\n",
      "P(digital | interact)=0.50\n",
      "P(content | digital)=1.00\n",
      "P(. | content)=0.67\n",
      "P(learn | model)=0.12\n",
      "P(pattern | learn)=1.00\n",
      "P(vast | pattern)=1.00\n",
      "P(datasets | vast)=1.00\n",
      "P(produce | datasets)=1.00\n",
      "P(original | produce)=1.00\n",
      "P(text | original)=1.00\n",
      "P(, | text)=0.67\n",
      "P(image | ,)=0.15\n",
      "P(, | image)=0.67\n",
      "P(code | ,)=0.08\n",
      "P(. | code)=1.00\n",
      "P(language | Large)=1.00\n",
      "P(model | language)=1.00\n",
      "P(like | model)=0.25\n",
      "P(GPT-4 | like)=0.25\n",
      "P(write | GPT-4)=1.00\n",
      "P(essay | write)=1.00\n",
      "P(, | essay)=0.50\n",
      "P(summarize | ,)=0.08\n",
      "P(document | summarize)=1.00\n",
      "P(, | document)=1.00\n",
      "P(even | ,)=0.08\n",
      "P(debug | even)=1.00\n",
      "P(software | debug)=1.00\n",
      "P(. | software)=1.00\n",
      "P(model | Diffusion)=1.00\n",
      "P(generate | model)=0.12\n",
      "P(photorealistic | generate)=1.00\n",
      "P(image | photorealistic)=1.00\n",
      "P(simple | image)=0.33\n",
      "P(text | simple)=1.00\n",
      "P(prompt | text)=0.33\n",
      "P(. | prompt)=1.00\n",
      "P(major | One)=1.00\n",
      "P(challenge | major)=1.00\n",
      "P(ensure | challenge)=1.00\n",
      "P(output | ensure)=1.00\n",
      "P(align | output)=0.50\n",
      "P(human | align)=1.00\n",
      "P(value | human)=0.50\n",
      "P(. | value)=1.00\n",
      "P(train | Bias)=1.00\n",
      "P(data | train)=0.50\n",
      "P(often | data)=0.50\n",
      "P(lead | often)=1.00\n",
      "P(harmful | lead)=1.00\n",
      "P(mislead | harmful)=1.00\n",
      "P(result | mislead)=1.00\n",
      "P(. | result)=1.00\n",
      "P(use | Researchers)=1.00\n",
      "P(techniques | use)=0.33\n",
      "P(like | techniques)=0.50\n",
      "P(RLHF | like)=0.25\n",
      "P(refine | RLHF)=1.00\n",
      "P(model | refine)=1.00\n",
      "P(behavior | model)=0.12\n",
      "P(feedback | behavior)=1.00\n",
      "P(. | feedback)=1.00\n",
      "P(issue | Another)=1.00\n",
      "P(computational | issue)=1.00\n",
      "P(costâ€ | computational)=1.00\n",
      "P(” | costâ€)=1.00\n",
      "P(train | ”)=0.33\n",
      "P(state-of-the-art | train)=0.50\n",
      "P(model | state-of-the-art)=1.00\n",
      "P(require | model)=0.12\n",
      "P(millions | require)=1.00\n",
      "P(dollars | millions)=1.00\n",
      "P(GPU | dollars)=1.00\n",
      "P(hours | GPU)=1.00\n",
      "P(. | hours)=1.00\n",
      "P(, | Despite)=1.00\n",
      "P(open-source | ,)=0.08\n",
      "P(alternatives | open-source)=1.00\n",
      "P(like | alternatives)=1.00\n",
      "P(LLaMA | like)=0.25\n",
      "P(make | LLaMA)=1.00\n",
      "P(technology | make)=0.50\n",
      "P(accessible | technology)=0.50\n",
      "P(. | accessible)=1.00\n",
      "P(leverage | Startups)=1.00\n",
      "P(generative | leverage)=1.00\n",
      "P(AI | generative)=1.00\n",
      "P(market | AI)=0.12\n",
      "P(copy | market)=1.00\n",
      "P(, | copy)=1.00\n",
      "P(video | ,)=0.08\n",
      "P(game | video)=1.00\n",
      "P(assets | game)=1.00\n",
      "P(, | assets)=1.00\n",
      "P(personalize | ,)=0.08\n",
      "P(education | personalize)=1.00\n",
      "P(tool | education)=1.00\n",
      "P(. | tool)=1.00\n",
      "P(argue | Critics)=1.00\n",
      "P(systems | argue)=1.00\n",
      "P(lack | systems)=0.50\n",
      "P(true | lack)=1.00\n",
      "P(understand | true)=1.00\n",
      "P(merely | understand)=1.00\n",
      "P(remix | merely)=1.00\n",
      "P(exist | remix)=1.00\n",
      "P(content | exist)=1.00\n",
      "P(dispute | Copyright)=1.00\n",
      "P(erupt | dispute)=1.00\n",
      "P(AI-generated | erupt)=1.00\n",
      "P(art | AI-generated)=1.00\n",
      "P(mimic | art)=1.00\n",
      "P(live | mimic)=1.00\n",
      "P(artistsâ€™ | live)=1.00\n",
      "P(style | artistsâ€™)=1.00\n",
      "P(. | style)=1.00\n",
      "P(, | Meanwhile)=1.00\n",
      "P(enterprises | ,)=0.08\n",
      "P(deploy | enterprises)=1.00\n",
      "P(chatbots | deploy)=1.00\n",
      "P(handle | chatbots)=1.00\n",
      "P(customer | handle)=1.00\n",
      "P(service | customer)=1.00\n",
      "P(mix | service)=1.00\n",
      "P(success | mix)=1.00\n",
      "P(. | success)=1.00\n",
      "P(” | Hallucinationsâ€)=1.00\n",
      "P(model | ”)=0.33\n",
      "P(invent | model)=0.12\n",
      "P(false | invent)=1.00\n",
      "P(factsâ€ | false)=1.00\n",
      "P(” | factsâ€)=1.00\n",
      "P(remain | ”)=0.33\n",
      "P(critical | remain)=1.00\n",
      "P(flaw | critical)=1.00\n",
      "P(medical | flaw)=1.00\n",
      "P(legal | medical)=1.00\n",
      "P(use | legal)=1.00\n",
      "P(case | use)=0.33\n",
      "P(. | case)=1.00\n",
      "P(draft | Governments)=1.00\n",
      "P(regulations | draft)=1.00\n",
      "P(address | regulations)=1.00\n",
      "P(deepfakes | address)=1.00\n",
      "P(misinformation | deepfakes)=1.00\n",
      "P(risk | misinformation)=1.00\n",
      "P(. | risk)=1.00\n",
      "P(side | creative)=1.00\n",
      "P(, | side)=1.00\n",
      "P(writers | ,)=0.08\n",
      "P(use | writers)=1.00\n",
      "P(AI | use)=0.33\n",
      "P(brainstorm | AI)=0.12\n",
      "P(rarely | brainstorm)=1.00\n",
      "P(publish | rarely)=1.00\n",
      "P(raw | publish)=1.00\n",
      "P(output | raw)=1.00\n",
      "P(. | output)=0.50\n",
      "P(assistance | line)=1.00\n",
      "P(replacement | assistance)=1.00\n",
      "P(spark | replacement)=1.00\n",
      "P(ethical | spark)=1.00\n",
      "P(debate | ethical)=1.00\n",
      "P(. | debate)=1.00\n",
      "P(worry | Educators)=1.00\n",
      "P(students | worry)=1.00\n",
      "P(outsource | students)=1.00\n",
      "P(essay | outsource)=1.00\n",
      "P(undetectable | essay)=0.50\n",
      "P(AI | undetectable)=1.00\n",
      "P(tool | AI)=0.12\n",
      "P(watermark | company)=1.00\n",
      "P(AI | watermark)=1.00\n",
      "P(content | AI)=0.12\n",
      "P(distinguish | content)=0.33\n",
      "P(human | distinguish)=1.00\n",
      "P(work | human)=0.50\n",
      "P(. | work)=1.00\n",
      "P(systems | Hybrid)=1.00\n",
      "P(combine | systems)=0.50\n",
      "P(symbolic | combine)=1.00\n",
      "P(logic | symbolic)=1.00\n",
      "P(neural | logic)=1.00\n",
      "P(network | neural)=1.00\n",
      "P(aim | network)=1.00\n",
      "P(improve | aim)=1.00\n",
      "P(reason | improve)=1.00\n",
      "P(. | reason)=1.00\n",
      "P(model | Multimodal)=1.00\n",
      "P(Gemini | like)=0.25\n",
      "P(process | Gemini)=1.00\n",
      "P(text | process)=1.00\n",
      "P(audio | ,)=0.08\n",
      "P(simultaneously | audio)=1.00\n",
      "P(. | simultaneously)=1.00\n",
      "P(translation | Real-time)=1.00\n",
      "P(apps | translation)=1.00\n",
      "P(preserve | apps)=1.00\n",
      "P(speaker | preserve)=1.00\n",
      "P(tone | speaker)=1.00\n",
      "P(emotion | tone)=1.00\n",
      "P(. | emotion)=1.00\n",
      "P(consumption | Energy)=1.00\n",
      "P(data | consumption)=1.00\n",
      "P(center | data)=0.50\n",
      "P(power | center)=1.00\n",
      "P(AI | power)=1.00\n",
      "P(another | AI)=0.12\n",
      "P(grow | another)=1.00\n",
      "P(concern | grow)=0.50\n",
      "P(. | concern)=1.00\n",
      "P(believe | Optimists)=1.00\n",
      "P(generative | believe)=1.00\n",
      "P(could | AI)=0.12\n",
      "P(democratize | could)=1.00\n",
      "P(creativity | democratize)=1.00\n",
      "P(, | creativity)=1.00\n",
      "P(skeptics | ,)=0.08\n",
      "P(fear | skeptics)=1.00\n",
      "P(job | fear)=1.00\n",
      "P(displacement | job)=1.00\n",
      "P(. | displacement)=1.00\n",
      "P(techniques | Explainability)=1.00\n",
      "P(attempt | techniques)=0.50\n",
      "P(make | attempt)=1.00\n",
      "P(black-box | make)=0.50\n",
      "P(model | black-box)=1.00\n",
      "P(transparent | model)=0.12\n",
      "P(. | transparent)=1.00\n",
      "P(grow | capabilities)=1.00\n",
      "P(, | grow)=0.50\n",
      "P(call | ,)=0.08\n",
      "P(global | call)=1.00\n",
      "P(standards | global)=1.00\n",
      "P(development | standards)=1.00\n",
      "P(deployment | development)=1.00\n",
      "P(. | deployment)=1.00\n",
      "P(frontier | next)=1.00\n",
      "P(embody | frontier)=1.00\n",
      "P(AI | embody)=1.00\n",
      "P(interact | AI)=0.12\n",
      "P(physical | interact)=0.50\n",
      "P(world | physical)=1.00\n",
      "P(. | world)=1.00\n",
      "P(technology | Whether)=1.00\n",
      "P(uplift | technology)=0.50\n",
      "P(humanity | uplift)=1.00\n",
      "P(depend | humanity)=1.00\n",
      "P(govern | depend)=1.00\n",
      "P(today | govern)=1.00\n",
      "P(. | today)=1.00\n"
     ]
    }
   ],
   "source": [
    "bigram_probalities={\n",
    "    bigram:count/word_counts[bigram[0]]\n",
    "    for bigram, count in bigram_counts.items()\n",
    "}\n",
    "print(\"\\nProcessed Sentences:\",processed_sentences)\n",
    "print(\"\\nBigram Probailites:\")\n",
    "for bigram,prob in bigram_probalities.items():\n",
    "    print(f\"P({bigram[1]} | {bigram[0]})={prob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82014a0-b7ed-42d9-b541-e5e47753b54e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
