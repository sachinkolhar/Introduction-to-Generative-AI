{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "88DQBHvtj7HG"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify.util import accuracy\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('movie_reviews')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A5Wh7Sjkj6B",
        "outputId": "599396db-4e37-4d10-c951-844a25cc0210"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_tokens = list(text)\n",
        "print(\"Character Tokens:\", char_tokens)\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\", word_tokens)\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"Sentence Tokens:\", sent_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj0ias8DkPCg",
        "outputId": "307b5659-13a7-4549-eb6d-53d952506f41"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character Tokens: ['A', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', ' ', 'I', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', ' ', '(', 'A', 'I', ')', ' ', 'i', 's', ' ', 't', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'i', 'n', ' ', 'r', 'e', 'm', 'a', 'r', 'k', 'a', 'b', 'l', 'e', ' ', 'w', 'a', 'y', 's', '.', ' ', 'F', 'r', 'o', 'm', ' ', 's', 'e', 'l', 'f', '-', 'd', 'r', 'i', 'v', 'i', 'n', 'g', ' ', 'c', 'a', 'r', 's', ' ', 't', 'o', ' ', 'v', 'i', 'r', 't', 'u', 'a', 'l', ' ', 'a', 's', 's', 'i', 's', 't', 'a', 'n', 't', 's', ' ', 'l', 'i', 'k', 'e', ' ', 'S', 'i', 'r', 'i', ' ', 'a', 'n', 'd', ' ', 'A', 'l', 'e', 'x', 'a', ',', ' ', 'A', 'I', ' ', 'i', 's', ' ', 'm', 'a', 'k', 'i', 'n', 'g', ' ', 'o', 'u', 'r', ' ', 'l', 'i', 'v', 'e', 's', ' ', 'e', 'a', 's', 'i', 'e', 'r', ' ', 'a', 'n', 'd', ' ', 'm', 'o', 'r', 'e', ' ', 'e', 'f', 'f', 'i', 'c', 'i', 'e', 'n', 't', '.', ' ', 'M', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ',', ' ', 'a', ' ', 's', 'u', 'b', 's', 'e', 't', ' ', 'o', 'f', ' ', 'A', 'I', ',', ' ', 'e', 'n', 'a', 'b', 'l', 'e', 's', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', 's', ' ', 't', 'o', ' ', 'l', 'e', 'a', 'r', 'n', ' ', 'f', 'r', 'o', 'm', ' ', 'd', 'a', 't', 'a', ' ', 'a', 'n', 'd', ' ', 'i', 'm', 'p', 'r', 'o', 'v', 'e', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'i', 'm', 'e', '.', ' ', 'N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'L', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'P', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', '(', 'N', 'L', 'P', ')', ',', ' ', 'a', 'n', 'o', 't', 'h', 'e', 'r', ' ', 'e', 'x', 'c', 'i', 't', 'i', 'n', 'g', ' ', 'f', 'i', 'e', 'l', 'd', ',', ' ', 'a', 'l', 'l', 'o', 'w', 's', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 's', ' ', 't', 'o', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', ' ', 'a', 'n', 'd', ' ', 'i', 'n', 't', 'e', 'r', 'a', 'c', 't', ' ', 'w', 'i', 't', 'h', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', '.', ' ', 'F', 'o', 'r', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ',', ' ', 'N', 'L', 'P', ' ', 'p', 'o', 'w', 'e', 'r', 's', ' ', 'c', 'h', 'a', 't', 'b', 'o', 't', 's', ',', ' ', 's', 'e', 'n', 't', 'i', 'm', 'e', 'n', 't', ' ', 'a', 'n', 'a', 'l', 'y', 's', 'i', 's', ',', ' ', 'a', 'n', 'd', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 't', 'r', 'a', 'n', 's', 'l', 'a', 't', 'i', 'o', 'n', '.', ' ', 'T', 'h', 'e', ' ', 'f', 'u', 't', 'u', 'r', 'e', ' ', 'o', 'f', ' ', 'A', 'I', ' ', 'i', 's', ' ', 'b', 'r', 'i', 'g', 'h', 't', ',', ' ', 'w', 'i', 't', 'h', ' ', 'e', 'n', 'd', 'l', 'e', 's', 's', ' ', 'p', 'o', 's', 's', 'i', 'b', 'i', 'l', 'i', 't', 'i', 'e', 's', ' ', 'f', 'o', 'r', ' ', 'i', 'n', 'n', 'o', 'v', 'a', 't', 'i', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 'g', 'r', 'o', 'w', 't', 'h', '.']\n",
            "Word Tokens: ['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'transforming', 'the', 'world', 'in', 'remarkable', 'ways', '.', 'From', 'self-driving', 'cars', 'to', 'virtual', 'assistants', 'like', 'Siri', 'and', 'Alexa', ',', 'AI', 'is', 'making', 'our', 'lives', 'easier', 'and', 'more', 'efficient', '.', 'Machine', 'learning', ',', 'a', 'subset', 'of', 'AI', ',', 'enables', 'computers', 'to', 'learn', 'from', 'data', 'and', 'improve', 'over', 'time', '.', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'another', 'exciting', 'field', ',', 'allows', 'machines', 'to', 'understand', 'and', 'interact', 'with', 'human', 'language', '.', 'For', 'example', ',', 'NLP', 'powers', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'language', 'translation', '.', 'The', 'future', 'of', 'AI', 'is', 'bright', ',', 'with', 'endless', 'possibilities', 'for', 'innovation', 'and', 'growth', '.']\n",
            "Sentence Tokens: ['Artificial Intelligence (AI) is transforming the world in remarkable ways.', 'From self-driving cars to virtual assistants like Siri and Alexa, AI is making our lives easier and more efficient.', 'Machine learning, a subset of AI, enables computers to learn from data and improve over time.', 'Natural Language Processing (NLP), another exciting field, allows machines to understand and interact with human language.', 'For example, NLP powers chatbots, sentiment analysis, and language translation.', 'The future of AI is bright, with endless possibilities for innovation and growth.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "print(\"Filtered Words (without stopwords):\", filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nafvIgRzkxyW",
        "outputId": "9cbbf6ec-5d2d-4283-aed7-88beb60e4be6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words (without stopwords): ['Artificial', 'Intelligence', '(', 'AI', ')', 'transforming', 'world', 'remarkable', 'ways', '.', 'self-driving', 'cars', 'virtual', 'assistants', 'like', 'Siri', 'Alexa', ',', 'AI', 'making', 'lives', 'easier', 'efficient', '.', 'Machine', 'learning', ',', 'subset', 'AI', ',', 'enables', 'computers', 'learn', 'data', 'improve', 'time', '.', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'another', 'exciting', 'field', ',', 'allows', 'machines', 'understand', 'interact', 'human', 'language', '.', 'example', ',', 'NLP', 'powers', 'chatbots', ',', 'sentiment', 'analysis', ',', 'language', 'translation', '.', 'future', 'AI', 'bright', ',', 'endless', 'possibilities', 'innovation', 'growth', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAjCyVDyk3OJ",
        "outputId": "bac49803-5d25-4289-9811-05778cc70c50"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['artifici', 'intellig', '(', 'ai', ')', 'transform', 'world', 'remark', 'way', '.', 'self-driv', 'car', 'virtual', 'assist', 'like', 'siri', 'alexa', ',', 'ai', 'make', 'live', 'easier', 'effici', '.', 'machin', 'learn', ',', 'subset', 'ai', ',', 'enabl', 'comput', 'learn', 'data', 'improv', 'time', '.', 'natur', 'languag', 'process', '(', 'nlp', ')', ',', 'anoth', 'excit', 'field', ',', 'allow', 'machin', 'understand', 'interact', 'human', 'languag', '.', 'exampl', ',', 'nlp', 'power', 'chatbot', ',', 'sentiment', 'analysi', ',', 'languag', 'translat', '.', 'futur', 'ai', 'bright', ',', 'endless', 'possibl', 'innov', 'growth', '.']\n",
            "Lemmatized Words: ['Artificial', 'Intelligence', '(', 'AI', ')', 'transforming', 'world', 'remarkable', 'way', '.', 'self-driving', 'car', 'virtual', 'assistant', 'like', 'Siri', 'Alexa', ',', 'AI', 'making', 'life', 'easier', 'efficient', '.', 'Machine', 'learning', ',', 'subset', 'AI', ',', 'enables', 'computer', 'learn', 'data', 'improve', 'time', '.', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'another', 'exciting', 'field', ',', 'allows', 'machine', 'understand', 'interact', 'human', 'language', '.', 'example', ',', 'NLP', 'power', 'chatbots', ',', 'sentiment', 'analysis', ',', 'language', 'translation', '.', 'future', 'AI', 'bright', ',', 'endless', 'possibility', 'innovation', 'growth', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = pos_tag(word_tokens)\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "pos_count = {}\n",
        "for word, tag in pos_tags:\n",
        "    pos_count[tag] = pos_count.get(tag, 0) + 1\n",
        "\n",
        "print(\"POS Tag Count:\", pos_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fxr_DfZk8jQ",
        "outputId": "2a665a00-a186-466f-ab3a-18d23237a8fc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('(', '('), ('AI', 'NNP'), (')', ')'), ('is', 'VBZ'), ('transforming', 'VBG'), ('the', 'DT'), ('world', 'NN'), ('in', 'IN'), ('remarkable', 'JJ'), ('ways', 'NNS'), ('.', '.'), ('From', 'IN'), ('self-driving', 'JJ'), ('cars', 'NNS'), ('to', 'TO'), ('virtual', 'JJ'), ('assistants', 'NNS'), ('like', 'IN'), ('Siri', 'NNP'), ('and', 'CC'), ('Alexa', 'NNP'), (',', ','), ('AI', 'NNP'), ('is', 'VBZ'), ('making', 'VBG'), ('our', 'PRP$'), ('lives', 'NNS'), ('easier', 'JJR'), ('and', 'CC'), ('more', 'RBR'), ('efficient', 'JJ'), ('.', '.'), ('Machine', 'NNP'), ('learning', 'NN'), (',', ','), ('a', 'DT'), ('subset', 'NN'), ('of', 'IN'), ('AI', 'NNP'), (',', ','), ('enables', 'VBZ'), ('computers', 'NNS'), ('to', 'TO'), ('learn', 'VB'), ('from', 'IN'), ('data', 'NNS'), ('and', 'CC'), ('improve', 'VB'), ('over', 'IN'), ('time', 'NN'), ('.', '.'), ('Natural', 'JJ'), ('Language', 'NN'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), (',', ','), ('another', 'DT'), ('exciting', 'JJ'), ('field', 'NN'), (',', ','), ('allows', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('and', 'CC'), ('interact', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('.', '.'), ('For', 'IN'), ('example', 'NN'), (',', ','), ('NLP', 'NNP'), ('powers', 'NNS'), ('chatbots', 'NNS'), (',', ','), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('and', 'CC'), ('language', 'NN'), ('translation', 'NN'), ('.', '.'), ('The', 'DT'), ('future', 'NN'), ('of', 'IN'), ('AI', 'NNP'), ('is', 'VBZ'), ('bright', 'JJ'), (',', ','), ('with', 'IN'), ('endless', 'JJ'), ('possibilities', 'NNS'), ('for', 'IN'), ('innovation', 'NN'), ('and', 'CC'), ('growth', 'NN'), ('.', '.')]\n",
            "POS Tag Count: {'JJ': 10, 'NNP': 11, '(': 2, ')': 2, 'VBZ': 5, 'VBG': 2, 'DT': 4, 'NN': 15, 'IN': 11, 'NNS': 10, '.': 6, 'TO': 3, 'CC': 6, ',': 9, 'PRP$': 1, 'JJR': 1, 'RBR': 1, 'VB': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [word_tokenize(sentence) for sentence in sent_tokenize(text)]\n",
        "\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "print(\"Words most similar to 'chatbots':\", model.wv.most_similar('chatbots', topn=5))\n",
        "print(\"Words most similar to 'language':\", model.wv.most_similar('language', topn=5))\n",
        "print(\"Words most similar to 'learning':\", model.wv.most_similar('learning', topn=5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7ME-pPIlAPU",
        "outputId": "71b9288b-2b99-4c4f-9f66-b49915d1b72b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words most similar to 'chatbots': [('another', 0.21590183675289154), ('virtual', 0.16217005252838135), ('understand', 0.16153617203235626), ('example', 0.158206045627594), ('in', 0.15734745562076569)]\n",
            "Words most similar to 'language': [('assistants', 0.28538039326667786), ('example', 0.23994435369968414), ('and', 0.19883963465690613), ('bright', 0.19002686440944672), ('analysis', 0.172149196267128)]\n",
            "Words most similar to 'learning': [('Processing', 0.2620536684989929), ('more', 0.2476443648338318), ('allows', 0.19585104286670685), ('endless', 0.19041407108306885), ('computers', 0.1840551644563675)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.shuffle(documents)\n",
        "\n",
        "def document_features(document):\n",
        "    return {word: True for word in document}\n",
        "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "train_set, test_set = featuresets[1500:], featuresets[:1500]\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "new_sentences = [\n",
        "    \"The team played exceptionally well and secured a spot in the playoffs!\",\n",
        "    \"Her dedication and hard work paid off when she won the gold medal.\",\n",
        "    \"The stadium was filled with cheering fans, creating an electric atmosphere.\",\n",
        "    \"He broke the world record in the 100-meter sprint, making history!\",\n",
        "    \"The community came together to support the local youth sports program.\",\n",
        "    \"The match was disappointing as the team lost by a large margin.\",\n",
        "    \"Injuries plagued the season, leaving many players sidelined.\",\n",
        "    \"The referee made several questionable calls that frustrated both players and fans.\",\n",
        "    \"The coach's poor strategy led to a series of defeats for the team.\",\n",
        "    \"The athlete faced criticism after failing to perform at the championship.\"\n",
        "]\n",
        "correct_predictions = 0\n",
        "for sentence in new_sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    features = document_features(words)\n",
        "    predicted = classifier.classify(features)\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Predicted Sentiment: {predicted}\")\n",
        "\n",
        "    expected = 'pos' if sentence in [\n",
        "        \"The team played exceptionally well and secured a spot in the playoffs!\",\n",
        "        \"Her dedication and hard work paid off when she won the gold medal.\",\n",
        "        \"The stadium was filled with cheering fans, creating an electric atmosphere.\",\n",
        "        \"He broke the world record in the 100-meter sprint, making history!\",\n",
        "        \"The community came together to support the local youth sports program.\"\n",
        "    ] else 'neg'\n",
        "\n",
        "    if predicted == expected:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy_percentage = (correct_predictions / len(new_sentences)) * 100\n",
        "print(f\"Accuracy: {accuracy_percentage}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLN7lXCylNFA",
        "outputId": "c61f5644-8d72-4938-bdca-5602d4352a24"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: The team played exceptionally well and secured a spot in the playoffs!\n",
            "Predicted Sentiment: pos\n",
            "Sentence: Her dedication and hard work paid off when she won the gold medal.\n",
            "Predicted Sentiment: pos\n",
            "Sentence: The stadium was filled with cheering fans, creating an electric atmosphere.\n",
            "Predicted Sentiment: pos\n",
            "Sentence: He broke the world record in the 100-meter sprint, making history!\n",
            "Predicted Sentiment: pos\n",
            "Sentence: The community came together to support the local youth sports program.\n",
            "Predicted Sentiment: pos\n",
            "Sentence: The match was disappointing as the team lost by a large margin.\n",
            "Predicted Sentiment: neg\n",
            "Sentence: Injuries plagued the season, leaving many players sidelined.\n",
            "Predicted Sentiment: pos\n",
            "Sentence: The referee made several questionable calls that frustrated both players and fans.\n",
            "Predicted Sentiment: neg\n",
            "Sentence: The coach's poor strategy led to a series of defeats for the team.\n",
            "Predicted Sentiment: neg\n",
            "Sentence: The athlete faced criticism after failing to perform at the championship.\n",
            "Predicted Sentiment: pos\n",
            "Accuracy: 80.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"The team played exceptionally well and secured a spot in the playoffs!\",\n",
        "    \"Her dedication and hard work paid off when she won the gold medal.\",\n",
        "    \"The stadium was filled with cheering fans, creating an electric atmosphere.\",\n",
        "    \"He broke the world record in the 100-meter sprint, making history!\",\n",
        "    \"The community came together to support the local youth sports program.\",\n",
        "    \"The match was disappointing as the team lost by a large margin.\",\n",
        "    \"Injuries plagued the season, leaving many players sidelined.\",\n",
        "    \"The referee made several questionable calls that frustrated both players and fans.\",\n",
        "    \"The coach's poor strategy led to a series of defeats for the team.\",\n",
        "    \"The athlete faced criticism after failing to perform at the championship.\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    features = document_features(word_tokenize(sentence))\n",
        "    prediction = classifier.classify(features)\n",
        "    print(f\"Sentence: {sentence}\\nPrediction: {prediction}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu1rg481lW3o",
        "outputId": "bd87bbb3-bc1a-41d0-b711-730b82cc48e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: The team played exceptionally well and secured a spot in the playoffs!\n",
            "Prediction: pos\n",
            "\n",
            "Sentence: Her dedication and hard work paid off when she won the gold medal.\n",
            "Prediction: pos\n",
            "\n",
            "Sentence: The stadium was filled with cheering fans, creating an electric atmosphere.\n",
            "Prediction: pos\n",
            "\n",
            "Sentence: He broke the world record in the 100-meter sprint, making history!\n",
            "Prediction: pos\n",
            "\n",
            "Sentence: The community came together to support the local youth sports program.\n",
            "Prediction: pos\n",
            "\n",
            "Sentence: The match was disappointing as the team lost by a large margin.\n",
            "Prediction: pos\n",
            "\n",
            "Sentence: Injuries plagued the season, leaving many players sidelined.\n",
            "Prediction: neg\n",
            "\n",
            "Sentence: The referee made several questionable calls that frustrated both players and fans.\n",
            "Prediction: pos\n",
            "\n",
            "Sentence: The coach's poor strategy led to a series of defeats for the team.\n",
            "Prediction: neg\n",
            "\n",
            "Sentence: The athlete faced criticism after failing to perform at the championship.\n",
            "Prediction: pos\n",
            "\n"
          ]
        }
      ]
    }
  ]
}