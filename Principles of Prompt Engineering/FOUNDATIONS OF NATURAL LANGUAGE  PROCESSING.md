- ## How NLP works:
	- **Text Preprocessing:** <font color="#ffff00">Preparing raw text </font>for analysis by cleaning and structuring it.
		- **Tokenization:** Splits text into smaller parts like words.
		    - **Word Tokenization:** Breaks text into individual words.
		    - **Sentence Tokenization:** Divides text into separate sentences.
		    - **Character Tokenization:** Splits text into individual characters.
		    - **Subword Tokenization:** Breaks words into smaller meaningful parts.
		    - **Byte-Pair Encoding (BPE):** A type of **subword tokenization** that **merges frequent byte pairs** iteratively.
		- ‚úÖ **Lowercasing** ‚Äì Standardizing text by <font color="#ffff00">converting all characters to lowercase.  </font>
		- ‚úÖ **Stop Word Removal** ‚Äì Removing common words (e.g., <font color="#ffff00">"the", "and", "is"</font>) that don‚Äôt add meaning.  
		- ‚úÖ **Stemming/Lemmatization** 
			- **Stemming:** <font color="#ffff00">Removes suffixes </font>from words to get a <font color="#ffff00">root form</font>,
			- **Lemmatization:** Converts words to their <font color="#ffff00">dictionary base form</font> (lemma)
		- ‚úÖ **Text Cleaning** ‚Äì Removing <font color="#ffff00">punctuation, special characters, and numbers.</font>
	- **Text Vectorization:** Converting <font color="#ffff00">text into numerical representations</font> for machine processing.
	    - **Bag of Words (BoW):** Counts how often each word appears in the text.
	    - **TF-IDF (Term Frequency-Inverse Document Frequency):** <font color="#ffff00">Weights words</font> by their importance in a document.
	    - **Word Embeddings (Word2Vec, GloVe, FastText):** Converts words into dense <font color="#ffff00">numerical vectors,</font>
	    - - **WORD EMBEDDING:** Represents words as numbers that show their meaning.
		    - **Word2Vec:** Uses neural networks to learn word meanings from context.
			    - **Semantic Relationships::** words with similar semantic relationships (meaning) are positioned at <font color="#ffff00">similar distances and directions</font> from each other.
			    - **Syntactic Relationships:** principle applies to other relationships as well, such as those based on<font color="#ffff00"> grammatical roles or word order. </font>
		    - **GloVe (Global Vectors for Word Representation):** Uses word co-occurrence to find word meanings.
		    - **BERT (Bidirectional Encoder Representations from Transformers):** 
			    - vector representations of words in a continuous space 
	- **Text Analysis:** Extracting <font color="#ffff00">meaning from vectorized text</font> to generate insights or responses.
	    - üìç **Named Entity Recognition (NER)** ‚Äì <font color="#ffff00">Identifies names</font> of people, places, organizations, etc.  
		- üìç **Sentiment Analysis** ‚Äì Determines the <font color="#ffff00">emotional tone</font> of the text (positive, negative, neutral).  
		- üìç **Syntax Parsing** ‚Äì Analyzes sentence structure to understand <font color="#ffff00">grammatical relationships.</font>

- **Parts of Speech Tagging:** 
    - It is the process of assigning a **grammatical category** (noun, verb, adjective, etc.) to each word in a sentence.
    - examples
		- **Noun** ‚Üí NN
		- **Pronoun** ‚Üí PRP
		- **Verb** ‚Üí VB
		- **Adjective** ‚Üí JJ
		- **Adverb** ‚Üí RB
		- **Preposition** ‚Üí IN
		- **Conjunction** ‚Üí CC

- **Text Classification:** process of categorizing text into predefined labels or categories.
    - **Sentiment Analysis:** Determines the emotion in the text.
    - **Spam Detection:** Identifies unwanted messages.
    - **Topic Categorization:** Sorts text by subject.
    - **Language Identification:** Figures out what language text is in.
    - **Intent Recognition:** Understands what a user wants from text.
    - **Document Classification:** Sorts whole documents into categories.
    - **News Classification:** Puts news articles into sections.
    - **Emotion Detection:** Identifies feelings in text.
    - **Review Classification:** Sorts reviews by features.
    - **Toxic Comment Detection:** Finds abusive or harmful text.


----


# **Natural Language Processing (NLP)**

### **Definition:**

NLP is a branch of **Artificial Intelligence (AI)** that enables computers to:

- **Understand**
- **Interpret**
- **Generate**  
    human language in a meaningful and useful way.

### **Interdisciplinary Nature:**

NLP is a **confluence** of:

- **Computer Science**
- **Artificial Intelligence**
- **Linguistics**

---

## **How NLP Works**

NLP operates in three major steps:

1. **Text Preprocessing**
2. **Text Vectorization**
3. **Training and Analysis**

---

## **1. Text Preprocessing**

Preparing raw text for analysis by cleaning and structuring it.

### **Key Techniques:**

‚úÖ **Tokenization** ‚Äì Breaking text into words, sentences, or phrases.  
‚úÖ **Lowercasing** ‚Äì Standardizing text by converting all characters to lowercase.  
‚úÖ **Stop Word Removal** ‚Äì Removing common words (e.g., "the", "and", "is") that don‚Äôt add meaning.  
‚úÖ **Stemming/Lemmatization** ‚Äì Reducing words to their root form (e.g., "running" ‚Üí "run").  
‚úÖ **Text Cleaning** ‚Äì Removing punctuation, special characters, and numbers.

---

## **2. Text Vectorization**

Converting text into numerical representations for machine processing.

### **Common Methods:**

üìå **Bag of Words (BoW)** ‚Äì Represents text as a collection of words, ignoring grammar but keeping word frequency.  
üìå **TF-IDF (Term Frequency-Inverse Document Frequency)** ‚Äì Weighs words based on their importance in a document relative to a corpus.  
üìå **Word Embeddings (Word2Vec, GloVe, FastText)** ‚Äì Converts words into dense numerical vectors, capturing word relationships and meanings.

---

## **3. Text Analysis**

Extracting meaning from vectorized text to generate insights or responses.

### **NLP Algorithms for Analysis:**

üìç **Named Entity Recognition (NER)** ‚Äì <font color="#ffff00">Identifies names</font> of people, places, organizations, etc.  
üìç **Sentiment Analysis** ‚Äì Determines the <font color="#ffff00">emotional tone</font> of the text (positive, negative, neutral).  
üìç **Syntax Parsing** ‚Äì Analyzes sentence structure to understand <font color="#ffff00">grammatical relationships.</font>

---

## **Conclusion**

NLP is a crucial AI field that helps computers understand and process human language. By using **preprocessing, vectorization, and analysis**, NLP powers applications like **chatbots, voice assistants, search engines, and sentiment analysis tools.**


# Tokenization

## **Definition:**

Tokenization is the process of breaking down a piece of text (sentence, paragraph, or document) into **smaller units called "tokens"**. These tokens can be words, sentences, characters, or subwords.

---

## **Types of Tokenization:**


|**Tokenization Type**|**Description**|**Example Input**|**Example Tokens**|
|---|---|---|---|
|**Word Tokenization**|Splitting text into individual words. Used for text analysis and NLP tasks.|`"Machine learning is fascinating."`|`["Machine", "learning", "is", "fascinating"]`|
|**Sentence Tokenization**|Splitting text into sentences. Useful for text summarization and machine translation.|`"Machine learning is fascinating. It has many applications."`|`["Machine learning is fascinating.", "It has many applications."]`|
|**Character Tokenization**|Splitting text into individual characters. Used in character-based NLP models, handwriting recognition, and language modeling.|`"Hello"`|`["H", "e", "l", "l", "o"]`|
|**Subword Tokenization**|Splitting text into meaningful subwords (prefixes, roots, suffixes). Helps in handling unknown words.|`"unhappiness"`|`["un", "happiness"]`|
|**Byte-Pair Encoding (BPE)**|A type of subword tokenization that merges frequent byte pairs iteratively. Used in modern NLP models like BERT and GPT.|`"lowering"`|`["low", "er", "ing"]`|

### **1. Word Tokenization**

- Splitting text into individual **words**.
- Used for text analysis and NLP tasks.
- **Example:**
    - **Input:** `"Machine learning is fascinating."`
    - **Tokens:** `["Machine", "learning", "is", "fascinating"]`

---

### **2. Sentence Tokenization**

- Splitting text into **sentences**.
- Useful for applications like **text summarization** and **machine translation**.
- **Example:**
    - **Input:** `"Machine learning is fascinating. It has many applications."`
    - **Tokens:** `["Machine learning is fascinating.", "It has many applications."]`

---

### **3. Character Tokenization**

- Splitting text into **individual characters**.
- Used in character-based NLP models, handwriting recognition, and language modeling.
- **Example:**
    - **Input:** `"Hello"`
    - **Tokens:** `["H", "e", "l", "l", "o"]`

---

### **4. Subword Tokenization**

- Splitting text into meaningful **subwords** (prefixes, roots, suffixes).
- Helps in handling **unknown words** and improving NLP model performance.
- **Example:**
    - **Input:** `"unhappiness"`
    - **Tokens:** `["un", "happiness"]`

---

### **5. Byte-Pair Encoding (BPE)**

- A type of **subword tokenization** that **merges frequent byte pairs** iteratively.
- Widely used in **modern NLP models** like BERT, GPT, and Transformers.
- **Example:**
    - **Input:** `"lowering"`
    - **Tokens:** `["low", "er", "ing"]`

---

### **Conclusion**

Tokenization is a **fundamental** step in NLP that converts raw text into structured tokens for further processing. The choice of tokenization technique depends on the application, language model, and task requirements.



## **1. Stop Word Removal**

### **Definition:**

Stop words are **commonly used words** (e.g., _the, is, in, a, an, and_) that are often **filtered out** in NLP to improve computational efficiency and focus on more meaningful words.

### **Why Remove Stop Words?**

- **Enhances text analysis** by eliminating words that do not contribute to meaning.
- **Improves efficiency** in NLP tasks like search engines and machine learning models.
- **Reduces dataset size**, making computations faster.

### **Example:**

- **Input:** `"Machine learning is fascinating."`
- **After Stop Word Removal:** `["Machine", "learning", "fascinating"]`

---

## **2. Stemming and Lemmatization**

### **Definition:**

Both techniques **reduce words to their root form**, but in different ways:

- **Stemming:** <font color="#ffff00">Removes suffixes </font>from words to get a <font color="#ffff00">root form</font>, but the result may not be a real word.
- **Lemmatization:** Converts words to their <font color="#ffff00">dictionary base form</font> (lemma), ensuring it is a valid word.

### **Why Use These Techniques?**

- **Enhances search engines** by matching different word forms.
- **Improves NLP models** by standardizing words for better text analysis.
- **Optimizes machine learning algorithms** by reducing vocabulary size.

---

## **3. Tokenization with Stop Word Removal, Stemming, and Lemmatization**

### **A. Tokenization with Stop Words Removal**

Splitting text into words while **removing stop words**.

- **Example:**
    - **Input:** `"Machine learning is fascinating."`
    - **Tokens:** `["Machine", "learning", "fascinating"]`

### **B. Tokenization with Lemmatization**

Splitting text into words and **converting each word to its dictionary form**.

- **Example:**
    - **Input:** `"Machines learning is fascinating."`
    - **Tokens:** `["machine", "learn", "be", "fascinating"]`

### **C. Tokenization with Stemming**

Splitting text into words and **reducing each word to its root form**.

- **Example:**
    - **Input:** `"Machines learning is fascinating."`
    - **Tokens:** `["machin", "learn", "is", "fascin"]`

---

## **4. Stemming vs. Lemmatization**

|Feature|**Stemming**|**Lemmatization**|
|---|---|---|
|**Definition**|Reduces words to a **stem** (root)|Converts words to a **lemma** (dictionary base form)|
|**Example**|`achieving ‚Üí achiev`|`achieving ‚Üí achieve`|
|**Output**|May not be a real word|Always a real word|
|**Context-Aware?**|No, it works on words individually|Yes, it uses context and grammar|
|**Speed**|Faster|Slower but more accurate|
|**Use Case**|Simple applications like search engines|Complex NLP tasks like text classification and AI chatbots|

---

|**Concept**|**Description**|**Example Input**|**Example Output**|
|---|---|---|---|
|**Stop Word Removal**|Removes common words that do not add meaning (e.g., _the, is, in, a, an, and_).|`"Machine learning is fascinating."`|`["Machine", "learning", "fascinating"]`|
|**Stemming**|Reduces words to their **root form** by removing suffixes, but the result may not be a real word.|`"Machines learning is fascinating."`|`["machin", "learn", "is", "fascin"]`|
|**Lemmatization**|Converts words to their **dictionary base form (lemma)**, ensuring the result is a valid word.|`"Machines learning is fascinating."`|`["machine", "learn", "be", "fascinating"]`|
|**Tokenization with Stop Word Removal**|Splits text into words while removing stop words.|`"Machine learning is fascinating."`|`["Machine", "learning", "fascinating"]`|
|**Tokenization with Lemmatization**|Splits text into words and converts them to dictionary forms.|`"Machines learning is fascinating."`|`["machine", "learn", "be", "fascinating"]`|
|**Tokenization with Stemming**|Splits text into words and reduces each word to its root form.|`"Machines learning is fascinating."`|`["machin", "learn", "is", "fascin"]`|
### **Conclusion**

- **Stop word removal** eliminates unnecessary words to improve text processing.
- **Stemming and Lemmatization** help reduce words to their base forms, making text analysis more efficient.
- **Stemming is faster but less accurate**, while **lemmatization is slower but ensures correct word forms**.

# **Parts of Speech (POS) Tagging in NLP**

## **1. Definition**

**Parts of Speech (POS) Tagging** is the process of assigning a **grammatical category** (noun, verb, adjective, etc.) to each word in a sentence. It is an essential step in **Natural Language Processing (NLP)** tasks such as **syntactic parsing, machine translation, and named entity recognition**.

### **Why is POS Tagging Important?**

- Helps in **understanding sentence structure** and meaning.
- Improves **machine translation and speech recognition**.
- Aids in **text-to-speech conversion**.
- Essential for **word sense disambiguation** (e.g., "bank" as a financial institution vs. riverbank).

---

## **2. Common Parts of Speech with Examples**

| **Part of Speech** | **Tag** | **Example Sentence**             |
| ------------------ | ------- | -------------------------------- |
| **Noun**           | NN      | "The **dog** barked loudly."     |
| **Pronoun**        | PRP     | "**He** is my friend."           |
| **Verb**           | VB      | "She **runs** every morning."    |
| **Adjective**      | JJ      | "The **blue** sky is beautiful." |
| **Adverb**         | RB      | "She sings **beautifully**."     |
| **Preposition**    | IN      | "The cat is **on** the roof."    |
| **Conjunction**    | CC      | "I like tea **and** coffee."     |

---

## **3. POS Tagging Techniques**

### **A. Rule-Based POS Tagging**

- Uses **handcrafted linguistic rules** to assign POS tags.
- Example: **"is" ‚Üí Verb (VB)**, **"quickly" ‚Üí Adverb (RB)**.

### **B. Statistical POS Tagging**

- Uses **probabilities and machine learning** models (e.g., Hidden Markov Model, Maximum Entropy Model).
- Example: A model learns that **"runs"** is usually a **verb** (VB) in **"She runs fast"**, but a **noun** (NNS) in **"The runs were exciting."**

### **C. Deep Learning-Based POS Tagging**

- Uses **Neural Networks** (e.g., LSTMs, BERT) for tagging.
- More accurate but requires large datasets for training.

---

## **4. Example of POS Tagging in Python (Using NLTK)**

```python
import nltk
from nltk.tokenize import word_tokenize
from nltk import pos_tag

# Sample sentence
sentence = "The quick brown fox jumps over the lazy dog."

# Tokenization
tokens = word_tokenize(sentence)

# POS tagging
pos_tags = pos_tag(tokens)

# Output
print(pos_tags)
```

### **Expected Output:**

```plaintext
[('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'),
 ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]
```

---

## **5. Conclusion**

- POS tagging is a **crucial step in NLP** that helps computers understand language structure.
- It can be done using **rule-based, statistical, or deep learning-based** methods.
- Python's **NLTK library** provides an easy way to perform POS tagging.

Would you like an example using **spaCy** or a more advanced deep-learning model? üòä


# **Text Classification in NLP**

## **1. Definition**

Text classification is the process of <font color="#ffff00">categorizing text into predefined labels or categories.</font> It is widely used in Natural Language Processing (NLP) applications such as **spam detection, sentiment analysis, intent recognition, and topic categorization**.

### **Why is Text Classification Important?**

‚úî Automates content organization.  
‚úî Enhances search engine results.  
‚úî Helps in spam filtering and content moderation.  
‚úî Improves chatbot responses and virtual assistants.

---

## **2. Common Types of Text Classification**

|**Category**|**Description**|**Example**|
|---|---|---|
|**Sentiment Analysis**|Determines the sentiment (positive, negative, neutral) in text.|"I love this product! It works great." ‚Üí **Positive**|
|**Spam Detection**|Identifies whether a message is spam or not.|"Win a free iPhone! Click here now!" ‚Üí **Spam**|
|**Topic Categorization**|Classifies text into predefined topics.|"The stock market is booming today." ‚Üí **Finance**|
|**Language Identification**|Detects the language of a text.|"Hola, ¬øc√≥mo est√°s?" ‚Üí **Spanish**|
|**Intent Recognition**|Recognizes user intent in chatbot queries.|"What's the weather like?" ‚Üí **Weather Inquiry**|
|**Document Classification**|Classifies entire documents into categories.|"This research explores deep learning." ‚Üí **Research Paper**|
|**News Classification**|Categorizes news articles into different sections.|"The national team won the championship." ‚Üí **Sports**|
|**Emotion Detection**|Identifies emotions like happiness, anger, sadness.|"I'm so frustrated!" ‚Üí **Anger**|
|**Review Classification**|Categorizes reviews based on quality, usability, etc.|"The battery life is amazing!" ‚Üí **Quality**|
|**Toxic Comment Detection**|Detects hate speech, abusive language.|"You are worthless and stupid!" ‚Üí **Toxic**|

---

## **3. How Text Classification Works**

### **Step 1: Text Preprocessing**

Before classifying text, we must clean and prepare the data:  
‚úî **Tokenization:** Splitting text into words.  
‚úî **Lowercasing:** Converting text to lowercase.  
‚úî **Stopword Removal:** Removing common words like "the," "is," etc.  
‚úî **Stemming/Lemmatization:** Converting words to their root form.

### **Step 2: Feature Extraction (Text Vectorization)**

To classify text, we convert words into numerical representations:

- **Bag of Words (BoW):** Counts word occurrences.
- **TF-IDF (Term Frequency-Inverse Document Frequency):** Weighs words based on importance.
- **Word Embeddings (Word2Vec, GloVe, BERT):** Captures semantic meaning in vector form.

### **Step 3: Model Training**

Machine learning algorithms used for classification:  
‚úî **Na√Øve Bayes** (good for spam filtering)  
‚úî **Support Vector Machines (SVM)** (effective in small datasets)  
‚úî **Random Forest** (used in ensemble learning)  
‚úî **Deep Learning Models (LSTMs, Transformers, BERT)** (used for complex NLP tasks)

---

## **4. Example: Sentiment Analysis in Python**

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# Sample training data
texts = ["I love this product!", "This is the worst experience.", "Absolutely amazing performance."]
labels = ["Positive", "Negative", "Positive"]

# Convert text to numerical representation
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

# Train Na√Øve Bayes model
model = MultinomialNB()
model.fit(X, labels)

# Test new text
test_text = ["I hate this service."]
X_test = vectorizer.transform(test_text)

# Predict class
prediction = model.predict(X_test)
print(prediction)
```

‚úî Expected Output: `["Negative"]`

---

## **5. Conclusion**

- Text classification helps in organizing, filtering, and analyzing text data.
- Various techniques like **machine learning** and **deep learning** can be used.
- Common applications include **spam detection, sentiment analysis, and news categorization**.

Would you like an example using **deep learning (BERT)** or more advanced classification techniques? üòä

# **Word Embeddings in NLP**

## **1. Definition**

- Word embeddings are vector representations of words in a continuous space where <font color="#ffff00">words with similar meanings have similar vector representations.</font> These embeddings help NLP models **capture semantic relationships** between words, enabling machines to **understand context and meaning** more effectively.

- ‚úî Converts words into numerical form for machine processing.  
- ‚úî Captures **relationships** between words (e.g., synonyms, analogies).  
- ‚úî Used in various NLP tasks like **text classification, machine translation, and sentiment analysis**.

---

## **2. Popular Word Embedding Techniques**

### **1. Word2Vec (Developed by Google)**

Word2Vec is a neural network-based model that learns word representations by analyzing the context in which words appear.

#### **How Word2Vec Works**

- It **trains on large text data** and learns to predict words based on surrounding words.
- It represents words as **dense vectors** in a high-dimensional space.
- Words appearing in similar contexts will have **similar embeddings**.

#### **Example**

üìå **Training Sentences:**  
üëâ "The cat sat on the mat."  
üëâ "The dog lay on the mat."

üìå **Learned Word Embeddings:**  
‚úî "cat" and "dog" will have similar vectors because they appear in **similar contexts**.

#### **Key Concept: Word Analogies**

![[Week-3.pptx.pdf#page=20&rect=89,84,781,391|Week-3.pptx, p.20]]



**Information Extracted:**

- **Analogy:**
    - The Euclidean distance (a measure of straight-line distance) between "Woman" and "Queen" is stated to be the same as the distance between "Man" and "King".
- **Algebraic Representation:**
    - This relationship is represented algebraically as:
        - Queen - Woman = King - Man
        - Which is then rewritten as: Queen - Woman + Man = King
- **Visual Representation:**
    - Two 3D graphs are shown:
        - One labeled "Semantic Relationship" with "Woman", "Queen", "Man", and "King" positioned in a way that visually represents the algebraic relationship.
        - One labeled "Syntactic Relationship" with "Biggest", "Big", "Small", and "Smallest" positioned similarly.

**Explanation and Notes:**

**1. Word Embeddings and Vector Space:**

- This example illustrates a core concept in word embeddings, particularly those like Word2Vec and GloVe. These models represent words as vectors in a high-dimensional space.
- **Semantic Relationships:**
    - The "Semantic Relationship" graph shows that <font color="#ffff00">words with similar semantic relationships (meaning) are positioned at similar distances and directions from each other.</font>
    - The analogy "Queen - Woman = King - Man" implies that the vector representing the change from "Woman" to "Queen" is analogous to the vector representing the change from "Man" to "King". Both represent a shift in gender and social status.
- **Syntactic Relationships:**
    - The "Syntactic Relationship" graph demonstrates that this principle applies to other relationships as well, such as those based on grammatical roles or word order. The relationship between "Big" and "Biggest" is similar to the relationship between "Small" and "Smallest", which both represent degrees of size.
- **Euclidean Distance:**
    - Euclidean distance is used to measure the "distance" between word vectors. A smaller distance indicates a stronger relationship.

**2. Algebraic Representation:**

- **Vector Arithmetic:**
    - The equations "Queen - Woman = King - Man" and "Queen - Woman + Man = King" are essentially vector arithmetic.
    - In the vector space, subtracting one word vector from another gives a vector that represents the relationship between those words.
    - Adding vectors combines their relationships.
- **Predicting Words:**
    - The equation "Queen - Woman + Man = King" shows how these models can be used to predict words. If you have the vectors for "Queen", "Woman", and "Man", you can perform the vector arithmetic to find a vector that is very close to the vector for "King".

**3. Implications:**

- **Meaning Representation:**
    - Word embeddings capture nuanced semantic and syntactic relationships between words.
- **Analogies:**
    - These models can solve analogies like "Man is to King as Woman is to ?" by performing vector calculations.
- **NLP Applications:**
    - Word embeddings are essential for various NLP tasks, including:
        - Text classification
        - Sentiment analysis
        - Machine translation
        - Information retrieval

**In Summary:**

The image effectively demonstrates how word embeddings represent semantic and syntactic relationships in a vector space. By using vector arithmetic and Euclidean distance, these models can capture and manipulate word meanings in a way that aligns with human intuition.


### **2. GloVe (Global Vectors for Word Representation)**

**Developed by Stanford**, GloVe is a **count-based** model that learns word embeddings by analyzing how frequently words co-occur in large corpora.

#### **How GloVe Works**

- It **constructs word vectors based on word co-occurrence** statistics in a text corpus.
- Unlike Word2Vec (which focuses on local context), **GloVe captures global word relationships**.
- It represents words as **vectors in continuous space** where angles and distances capture relationships.

#### **Example**

üìå **Training Sentences:**  
üëâ "Ice cream is cold."  
üëâ "Fire is hot."

üìå **Learned Word Embeddings:**  
‚úî "Cold" and "Hot" will have **opposite** vectors since they are antonyms.

##### **Co-occurrence Probability Example:**

- Probability of the word **solid** appearing near **ice** is **higher** than near **steam**.
- Probability of the word **solid** appearing near **fashion** is **low** (since they are unrelated).
- This statistical approach helps create **meaningful word embeddings**.

![[Week-3.pptx.pdf#page=22&rect=148,215,684,337|Week-3.pptx, p.22]]


**Information Extracted:**

- **Context:** The image is illustrating how the co-occurrence probability ratio works in GloVe (Global Vectors for Word Representation), a popular word embedding technique in Natural Language Processing (NLP).
- **Concepts:**
    - **P(k|ice):** The probability of word 'k' occurring in the context of the word 'ice'.
    - **P(k|steam):** The probability of word 'k' occurring in the context of the word 'steam'.
    - **P(k|ice) / P(k|steam):** The ratio of these probabilities.
- **Specific Words (k):** 'solid', 'gas', 'water', and 'fashion'.
- **Probabilities:** The image provides the probabilities of these words occurring in the context of 'ice' and 'steam'.
- **Ratios:** The image also shows the calculated ratios of these probabilities.

**Explanation:**

**1. Probability of Co-occurrence:**

- The probabilities P(k|ice) and P(k|steam) represent how often a word 'k' (e.g., 'solid') appears in the vicinity of 'ice' and 'steam', respectively, in a large corpus of text.
- For example, P(solid|ice) = 1.9 x 10‚Åª‚Å¥ means that the word 'solid' appears near the word 'ice' with a probability of 0.00019.

**2. Probability Ratio:**

- The ratio P(k|ice) / P(k|steam) is crucial in GloVe. It helps to understand the relative relationship between the words 'ice', 'steam', and 'k'.


> [!NOTE] **Interpretation
> - If the ratio is much greater than 1, it suggests that 'k' is more related to 'ice' than to 'steam'.
> - If the ratio is much less than 1, it suggests that 'k' is more related to 'steam' than to 'ice'.
> - If the ratio is close to 1, it suggests that 'k' is equally related (or unrelated) to both 'ice' and 'steam'.



**3. Analysis of the Example:**

- **Solid:**
    - The ratio for 'solid' is 8.9, which is much greater than 1.
    - This indicates that 'solid' is much more likely to appear in the context of 'ice' than 'steam', which aligns with our understanding that ice is a solid.
- **Gas:**
    - The ratio for 'gas' is 8.5 x 10‚Åª¬≤, which is much less than 1.
    - This indicates that 'gas' is much more likely to appear in the context of 'steam' than 'ice', which aligns with our understanding that steam is a gas.
- **Water:**
    - The ratio for 'water' is 1.36, which is close to 1.
    - This indicates that 'water' is somewhat related to both 'ice' and 'steam', which makes sense as both are forms of water.
- **Fashion:**
    - The ratio for 'fashion' is 0.96, which is very close to 1.
    - This indicates that 'fashion' is not particularly related to either 'ice' or 'steam', which also aligns with our intuition.

**4. GloVe and Word Embeddings:**

- GloVe uses these co-occurrence statistics to learn word embeddings, which are vector representations of words.
- The ratios help the model to capture the semantic relationships between words.
- Words with similar contexts will have similar vector representations.

**In essence, this example demonstrates how GloVe leverages co-occurrence probabilities and their ratios to understand and represent the relationships between words.**
### **3. BERT (Bidirectional Encoder Representations from Transformers)**

- **Developed by Google**, BERT is a **transformer-based model** that learns **contextualized word embeddings**. Unlike Word2Vec and GloVe (which generate fixed vectors for words), **BERT assigns different vectors to the same word based on its context**.

#### **How BERT Works**

- Uses **transformers** to analyze words in **both left and right** contexts (hence, **bidirectional**).
- **Understands different meanings of the same word** based on context.
- Excels in **question answering, sentiment analysis, and text classification**.

#### **Example**

- üìå **Sentence 1:** _"He went to the **bank** to deposit money."_  
- üìå **Sentence 2:** _"The river **bank** was full of trees."_

	- ‚úî **Traditional models (Word2Vec, GloVe)** ‚Üí Assign **the same vector** to "bank" in both sentences.  
	- ‚úî **BERT** ‚Üí Assigns **different vectors** to "bank" based on the context (**finance vs. nature**).

---

## **Comparing Word Embedding Techniques**

| **Technique** | **How it Works**                                                                                                                                                                                                                                                   | Examples                                                                                                                                                                                                                                                                                                                                                            | **Context-Aware?** | **Captures Word Relations?** | **Example Usage**                      |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------ | ---------------------------- | -------------------------------------- |
| **Word2Vec**  | Learns word vectors from <font color="#ffff00">local context </font>(neighboring words).                                                                                                                                                                           | üëâ "The cat sat on the mat."  <br>üëâ "The dog lay on the mat."<br><br>üìå **Learned Word Embeddings:**  <br>‚úî "cat" and "dog" will have similar vectors because they appear in **similar contexts**.                                                                                                                                                                 | ‚ùå No               | ‚úÖ Yes                        | Search engines, recommendation systems |
| **GloVe**     | Uses <font color="#ffff00">co-occurrence </font>statistics to model <font color="#ffff00">global word relationships.</font>                                                                                                                                        | **Co-occurrence Probability Example:**<br><br>- Probability of the word **solid** appearing near **ice** is **higher** than near **steam**.<br><br>- Probability of the word **solid** appearing near **fashion** is **low** (since they are unrelated).                                                                                                            | ‚ùå No               | ‚úÖ Yes                        | Topic modeling, sentiment analysis     |
| **BERT**      | **transformer-based model** that learns <font color="#ffff00">contextualized word embeddings. </font><br><br>Unlike Word2Vec and GloVe (which generate fixed vectors for words), <br><br>**BERT assigns different vectors to the same word based on its context**. | - üìå **Sentence 1:** _"He went to the **bank** to deposit money."_  <br>- üìå **Sentence 2:** _"The river **bank** was full of trees."_<br><br>	- ‚úî **Traditional models (Word2Vec, GloVe)** ‚Üí Assign **the same vector** to "bank" in both sentences.  <br>	<br>‚úî **BERT** ‚Üí Assigns **different vectors** to "bank" based on the context (**finance vs. nature**). | ‚úÖ Yes              | ‚úÖ Yes                        | Chatbots, machine translation          |

---

## **4. Conclusion**

‚úî **Word embeddings improve NLP models** by converting words into numerical vectors.  
‚úî **Word2Vec** is great for capturing **semantic relationships** between words.  
‚úî **GloVe** models **global** word associations using co-occurrence statistics.  
‚úî **BERT** is the most advanced, generating **context-aware** embeddings.

üöÄ **Want to implement word embeddings in Python?** Let me know! üòä
